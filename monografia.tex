\documentclass[tcc,capa]{texufpel}

\usepackage[latin1]{inputenc} % acentuacao
\usepackage{graphicx} % para inserir figuras
\usepackage{times,amsmath,amsfonts,xspace,dsfont,pgf,latexsym,amssymb}
\usepackage{todonotes}
\usepackage{caption}
\captionsetup{compatibility=false}
\usepackage{subcaption}
\usepackage[linesnumbered]{algorithm2e}

\DeclareCaptionType{myeqs}[Equation][List of equations]
\captionsetup[myeqs]{labelformat=simple}

\addtolength{\jot}{0.2cm}

\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line

\newcommand{\red}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\blue}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\DeclareMathSymbol{\Theta}{\mathalpha}{operators}{2}
\newcommand{\BigO}{\ensuremath{\operatorname{O}}}

\unidade{Centro de Desenvolvimento Tecnológico}
\curso{Ciência da Computação}
\nomecurso{Bacharelado em Ciência da Computação}
\titulocurso{Bacharel em Ciência da Computação}
\title{Int-DWTs Library - Algebraic Simplifications \\ Increasing Performance and Accuracy\\ of Discrete Wavelet Transforms}

\author{dos Santos}{Vinícius Rodrigues}
\advisor[Profa.~Dra.]{Reiser}{Renata Hax Sander}
\coadvisor[Prof.~Dr.]{Pilla}{Maurício}
\collaborator[Profa.~Dra.]{Kozakevicius}{Alice}

\keyword{wavelet}
\keyword{interval}
\keyword{performance}
\keyword{optimization}

\begin{document}

%\renewcommand{\advisorname}{Orientadora}           %descomente caso tenhas orientadora
%\renewcommand{\coadvisorname}{Coorientadora}      %descomente caso tenhas coorientadora

\maketitle 

\sloppy

\begin{abstract}

Este trabalho descreve a modelagem e o desenvolvimento de metodologia bem como a análise de resultados para consolidação da biblioteca Int-DWTs, contendo Métodos Intervalares para Transformadas Discretas Wavelet. Os métodos presentes em Int-DWTs incluem estensões intervalares e suas correspondentes otimizações para Transformadas Discretas Wavelet (TDWs), mais especificamente para as Transformadas Wavelet de Haar e Daubechies. As otimizações implementadas promoveram simplificações algébricas visando o incremento de desempenho e a garantia de exatidão das TDWs estudadas. São apresentadas e comparadas as análises de complexidade dos algoritmos desenvolvidos em relação aos presentes na literatura. A qualidade de resultados é verificada ao comparar três métricas: $EUC$ (Euclidean Distance), $MSE$ (\textit{Mean Squared Error}) e $PSNR$ (\textit{peak signal-to-noise ratio}). O nível de erro produzido pelas formulações intervalares, como também os desempenhos obtidos através das simplificações propostas, são analisadas e comparadas com resultados baseados na literatura. Em contextos onde as escalas de representação são relevantes para o problema, os ganhos de exatidão obtidos com as otimizações propostas representam uma contribuição significativa para a área de Computação Científica.

%são implementadas utilizando a biblioteca de aritimética intervalar C-XSC.

\end{abstract}

\begin{englishabstract}%
  {Algebraic Simplifications Increasing Performance And Accuracy of Discrete Wavelet Transforms}%
  {wavelet, interval, performance, optimization, haar, daubechies, accuracy, accuracy}
  
This project describes the main results consolidating the Interval Methods of Discrete Wavelet Transforms Library (Int-DWTs), providing algebraic simplifications in order to increase performance and guarantee accuracy of Discrete Wavelet Transforms (DWTs). The methods in the Int-DWTs include interval extensions and corresponding optimizations to the Haar Wavelet Transform (HWT) and Daubechies Wavelet Transform (DWT), which are implemented by using C-XSC. Optimizations for the normalized formulations of the one- and two-dimensional transforms to increase speed and guarantee accuracy of results are also presented. As an application, image compression is addressed, whose quality is computed and compared by different metrics. The error analysis of the different interval formulations as well as the speedup obtained through the interval formulations and the proposed simplifications are analyzed. The results consider a study over the interval extension of DWT, deducting simplifications and implementing optimizations based on the analogous methodology performed over HWT approaches. In addition, not only interval algorithms are developed but also metrics for evaluating procedure's accuracy are considered. By assuming concepts from Interval Mathematics, this work enables us to perform interval analysis and efficiently manage computing errors of DWTs. The metrics being used to measure result quality in the Int-DWTs are the following: Euclidean Distance, Mean Squared Error and Peak signal-to-noise ratio. In contexts where the scales of representation are relevant to the problem, the accuracy gain obtained with the proposed optimizations in the Int-DWTs represent a significant contribution to the scientific computing research area. 

\end{englishabstract}

%Lista de Figuras
\listoffigures

%Lista de Tabelas
%\listoftables

%lista de abreviaturas e siglas
\begin{listofabbrv}{SPMD}
        \item[DWTs] Discrete Wavelet Transforms
        \item[DaWT] Daubechies Wavelet Transform
        \item[HWT] Haar Wavelet Transform
        \item[EUC] Euclidean Distance
        \item[MSE] Mean Squared Error
        \item[PSNR] Peak Signal to Noise Ratio
        \item[PDE] Partial Differential Equations
\end{listofabbrv}

%Sumario
\tableofcontents

\chapter{Introduction}

The quality of numerical results in Scientific Computing (SC) depends on understanding the different error causes and on controlling their propagation,  as well as improving  computations upon the involved procedures.

This study considers the Interval Mathematics (IM) approach and proposes a solution based on Moore's arithmetic~\cite{Moore79} for the interval implementation of the Daubechies Wavelet Transform (DaWT) and Haar Wavelet Transform (HWT), considering its two approaches, Cascade and À-Trous, and their standard formulations. The implementation of interval extensions of both HWT and DaWT is the first step on the development of the Int-DWT library that will provide interval results for several Discrete Wavelet Transforms (DWTs).

Interval results carry over the safety of their quality together with the degree of their uncertainty~\cite{Moore79}. The diameter of the interval solution represents with fidelity the uncertainties of input parameters, being also an indicative of the error influence and of the extension of its propagation within the incoming data. Interval solutions also indicate truncation and rounding errors contained in the computed results.

The C-XSC library~\cite{holbig05} is employed to support the interval implementation of all formulations addressed by the Int-DWTs library. Such open source  library consists of a set of tools for the development of numerical algorithms, integrating precision and automatic verification of the results, frequently applied in SC.

In the last decade, many studies associating wavelet transforms with interval mathematics have arisen, promoting a new research direction and pointing out the relevance of  interval computations for a wide range of applications. Shu-Li et al.~\cite{shu-li2005} propose the interval extension of the interpolating wavelet family for the construction of an adaptive algorithm for solving Partial Differential Equations (PDE).

Another collocation method for solving PDE is presented by Liu~\cite{liu2013}, based on the interval extension for the Shannon-Garbor wavelet family. With respect to image processing applications, Minamoto and Aoki~\cite{minamoto2010} propose a blind digital image watermarking method using interval Daubechies wavelets.

The main motivation for interval techniques integrated with  DWTs is to provide trustworthy and validated results to technological and SC applications assuming such transformations. The pool of techniques involving wavelets is ample, specially in signal and image processing~\cite{5,6}.

The  HWT, the  simplest Discrete Wavelet Transform~(DWT) due to the complexity of its filters, is still nowadays largely explored,  being its robustness with respect to many different mathematical structures and space formulations  a relevant aspect~\cite{novikov2012} addressed in many applications. Besides, as demonstrated in \cite{brito1998}, the imaging inaccuracy of the Haar wavelet basis is the smallest possible, motivating its usage in many 2D problems as well as the current formulation in the interval context.

Hence, the present study introduces an integrated analysis of the original HWT algorithms and the proposed interval extensions.  Preliminary results of the new interval extension of the HWT presented in~\cite{7} are revisited here, including the interval extensions of the decimated HWT~\cite{stollnitz1995} (Cascade algorithm) for the 1D and 2D cases.

The current research addresses as well the {\it À-Trous} algorithm, an undecimated HWT formulation ~\cite{IEEE2007,kozakevicius2013}, proposing interval extensions for both  1D and 2D cases. Furthermore, a threshold procedure for treating signals and images, considered as a significant part of many compression and filtering algorithms, is analyzed and its interval extensions are presented, namely the Hard and Soft Decision procedures. 

During the study of the original normalized and non-normalized forms of the HWT \cite{stollnitz1995}, the possibility to optimize them arises from the observation of patterns of normalization factors in each level of transformation. Therefore, algebraic simplifications are executed to eliminate operations with irrational numbers, originally responsible for the normalization of the transform and considered in each iteration of the original algorithms \cite{weit2013}.

Through these simplifications, a gain of accuracy is obtained for the normalized approach of the decimated HWT, producing therefore more reliable results. In the current work, a similar heuristic is considered to simplify the standard formulation of the undecimated HWT and DaWT. This work also describes the complexity of the developed algorithms for both HWT and DaWT comparing with the ones from the literature.

This work is organized as follows. Chapter~\ref{sec:intarit} summarizes aspects about interval arithmetics and the metrics used to validate the results. Chapter~\ref{sec:DWTs} presents the original formulations of both HWT and DaWT. Chapter~\ref{sec:optintform} details the development of the proposed optimizations. Chapter~\ref{sec:tests} shows the configuration of tests and the analysis of results. Finally, conclusions are presented in Chapter~\ref{sec:conclusion}.

\chapter{Interval Mathematics}\label{sec:intarit}

This Chapter presents the fundamental concepts of Interval Mathematics, by describing its interval aritmethics in Section~\ref{sec:int-arit}, and introduces interval extensions of several metrics related to image quality evaluation in Section~\ref{sec:metrics}. In addition, the C-XSC library for computation of interval arithmetics is introduced in Section~\ref{sec:intext}.

\section{Concept of Interval Aritmethics}\label{sec:int-arit}

Interval analysis was developed by Ramon Moore in the 1960's~\cite{Moore59a,MoorePhD}, when computers were still at an early stage of development and every additional cost associated with keeping track of the computational errors were considered as too high. Furthermore, the produced error bounds were overly pessimistic and therefore quite useless.

Nowadays, the research for new interval methods have reached a high scientific level, producing tight error bounds faster than approximations of non-rigorous computations. Even in pure mathematics, non-trivial results have recently been proved using computer-aided methods based on interval techniques~\cite{Tucker11}.

Additionally, scientific computations demanding rigor as well as speed from numerical computations should be performed based on  techniques for performing validated numerical calculations provided by interval methods.

Distinct approaches of arithmetic operations have been discussed in the literature, see for example \cite{Walster97a,Hickey2001a} and~\cite{FS04}.

Despite distinct approaches of arithmetic for  intervals, see e.g.~\cite{Walster97a,Hickey2001a} and~\cite{FS04}, this paper considers the interval analysis proposed by Moore in the 1960's, providing methods to obtain accuracy in numerical calculations~\cite{Moore79,Moore59a,MoorePhD}.
%
%\subsection{Moore Interval Arithmetic Operations}\label{subsec:Moore}
%
%\red{Temos mesmo que explicar tudo isso? Não podemos citar o trabalho quando necessário? Pilla}
%

In Moore arithmetic, an interval $X$ is a continuum subset of real numbers which is in the infimum-supremum representation given as:
\begin{equation}
   X=\{x\in \mathbb{R} \colon a \leq x \leq b, a,b \in \mathbb{R}, a \leq b\}.
\end{equation}
%and in the midpoint-radius representation, expressed by:
%\begin{equation}
%   X=\{x\in \mathbb{R} \colon | x- a | \leq \alpha, a \in \mathbb{R},  0 \leq \alpha  \in \mathbb{R}\}.
%   \end{equation}
%
%The two representations are identical for real intervals but not for floating-point intervals.  The infimum-supremum arithmetic is used in computers, since arithmetic based on midpoint-radius representation demands overestimation for multiplication and division. In general, it is not exactly representable in floating point causing additional computational effort.
The set of all real intervals is indicated by $\mathbb{I}\mathbb{R}$. Frequently, an interval $X\in \mathbb{I}\mathbb{R}$ is indicated by its endpoints, $X=[a,b]$.  When $a=b$ then $[a,b] \in \mathbb{I}\mathbb{R}$ is called a degenerate interval. For an unary operator $\omega:\mathbb{I}\mathbb{R} \rightarrow \mathbb{I}\mathbb{R}$, we have that $\omega(X)= \{\omega(x) : x \in X\}$.
For an arithmetic operation $\ast: \mathbb{I}\mathbb{R}^2 \rightarrow \mathbb{I}\mathbb{R}$ such that $\ast \in \{ +,-, /, \cdot \}$,
the following property is valid for any two intervals $X$ and $Y$:
$X\ast Y =\{x \ast y \colon x \in X \mbox { and } y \in Y\}$.

Interval functions can be computed by performing arithmetic operations or by applying rational approximation methods~\cite{Walster97a}. The former identifies the class of rational interval functions and the latter, the class of irrational interval functions.
Thus, Moore arithmetic guarantees correctness in the sense that any computation performed with standard floating-point methods can also be done with their interval version. By considering the set $\Re = [-\infty, \infty]$ of extended real numbers,  the related family of subintervals of $[a,b]$ is given as

\begin{equation}\label{eq-3}
\mathbb{I}_{[a,b]} = \{[x,y]  \subseteq \Re \colon  a\leq x\leq y\leq b \}.
\end{equation}
In particular, let $U$ be the real unit interval $U=[0,1]\subseteq \Re$.
By Eq.~(\ref{eq-3}), the set of all subintervals of $U$ is indicated as
$\mathbb{I}_U=\{[x,y]\subseteq \Re \colon 0\leq x\leq y\leq 1\}$.
The  projections $l,r:\mathbb{I}_{[a,b]}\rightarrow [a,b]$  are respectively given by
$l([x,y])=x$ and  $r([x,y])=y$, for all  $[x,y] \in \mathbb{I}_{[a,b]}$.
Additionally, when $X=[x,y] \in \mathbb{I}_{[a,b]}$, the projection functions  $l(X)$ and $r(X)$ are also denoted by $\underline{X}$ and $\overline{X}$,
respectively. Thus, for all $X,Y \in \mathbb{I}_{[a,b]}$, interval arithmetic operations can be defined as follows:
\begin{eqnarray*}
    X\pm Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm} [\underline{X}\pm \underline{Y},\overline{X}\pm \overline{Y}]; \hspace{3.0cm} 
%     X-Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm}[\underline{X}-\overline{Y},\overline{X}-\underline{Y}] ;\\
      1/Y \hspace{-0.1cm}=\hspace{-0.1cm} [1/\overline{Y},1/\underline{Y}] , \mbox { \,\,\,\, if \,\, $0 \notin Y$};\\
      X\cdot Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm} [\min\ \{\underline{X}\cdot\underline{Y},\underline{X}\cdot\overline{Y},
\overline{X}\cdot\underline{Y},\overline{X}\cdot\overline{Y}\},\max
\{\underline{X}\cdot\underline{Y},\underline{X}\cdot\overline{Y},
\overline{X}\cdot\underline{Y},\overline{X}\cdot\overline{Y}\}].
\end{eqnarray*}
%
%Other operators considered in this study are described below:
%Let $X\in \mathbb{I}_{[a,b]} $ and $n\in \mathbb{N}$. 
Additionally, the power and root operations are, respectively,  defined as follows:
\begin{align*}
 X^n &= \left \{
\begin{array}{ll}
[\underline{X}^n, \overline{X}^n], & \mbox{ if } \underline{X} > 0 \mbox{ or } n \mbox{  is odd;} \\ \relax
%
[\overline{X}^n, \underline{X}^n], & \mbox{ if } \overline{X} < 0 \mbox{ and } n \mbox{  is even;} \\ \relax
%
[0, \left( \max\{\underline{X}, \overline{X}\} \right)^n], & \mbox{ if } \ 0 \in X \mbox{ and }   n \mbox{  is even}.
\end{array}
\right. \\
%
\sqrt[n]{X} &= \left \{
\begin{array}{ll}
[\sqrt[n]{\underline{X}}, \sqrt[n]{\overline{X}}], & \mbox{ if } \underline{X} > 0 \mbox{ or } n \mbox{  is odd;} \\ \relax
%
\mbox{indefined, \hspace{1.7cm}} & \mbox{otherwise.} \relax
\end{array}
\right.
\end{align*}

The partial order used here in the context of interval mathematics is the Product (or Kulisch-Miranker) order and, for all $X,Y \in \mathbb{I}_{[a,b]}$, it is defined as follows:
\begin{equation}
X\leq_{\mathbb{I}_{[a,b]}} Y \mbox{ iff } \underline{X}\leq
\underline{Y} \mbox{ and } \overline{X}\leq \overline{Y}.
\end{equation}
Additionally, an interval function preserving the partial order $\leq_{\mathbb{I}_{[a,b]}}$ is called $\mathbb{I}_{[a,b]}$-monotonic function with respect to the partially ordered set $(\mathbb{I}_{[a,b]}, \leq_{\mathbb{I}_{[a,b]}})$.

%%%%%%%%%%%%%%
% \subsection{Interval Extensions and Representations}
% %%%%%%%%%%%%%%
% It is possible to extend all standard (exponential, logarithm, trigonometric) real function $f$  to an interval function $F$. As pointed out in~\cite{Tucker11}, elementary functions, which are obtained by combining  constants, arithmetic operations and  compositions of standard functions, do not provide, in general, well-defined interval extensions. This can be observed for a set of singular points of a real function $f$, which frequently can be  included in the corresponding  natural extension. However,  the interval evaluation $F(X)$ produces the exact range of $f$ over the domain $X \in\mathbb{I}_{[a,b]}$.

% By the Fundamental Theorem of Interval Analysis\cite{Tucker11},
% let $f$ be an elementary function  and $F$ be an interval extension  such that $F(X)$ is well-defined, for $X \in\mathbb{I}_{[a,b]}$. Then, $F$ satisfies the following properties:
% \begin{description}
% \item [(i)] \textbf{Inclusion Monotonicity}: $Z \subseteq Z' \subseteq X \rightarrow F(Z) \subseteq F(Z') \subseteq F(X)$ ;
% \item [(ii)] \textbf{Range Enclosure}: $f[X] = f(X)$.
% \end{description}


% An interval $X \in\mathbb{I}_{[a,b]}$ is said to be an interval representation of a real number $\alpha$, if $\alpha \in X$.
% %Considering two interval representations $X$ and $Y$ of a real number $
% %\alpha$,  $X$ is said to be a better representation of
% %$\alpha$ than $Y$ if $X$ is narrower than $Y$, that is,  if $X\subseteq Y%$.
% This notion can be easily extended for
% tuples of $n$ intervals $(\vec{X}) = (X_1, \ldots, X_n)$.
% In~\cite{Moore59a}, a function $F:\mathbb{I}_{[a,b]}^n \rightarrow  \mathbb{I}_{[a,b]}$ is an \textbf{interval
% representation} of a function $f:[a,b]^n \rightarrow  [a,b]$ if, for each
% $\vec{X}\in {\mathbb{I}_{[a,b]}}^n$ and $\vec{x}\in \vec{X}$, $f(\vec{x})\in
% F(\vec{X})$.
% From the previous paragraph, an interval function may be seen as a representation of a subset of real numbers $X$ defined as the range $f(X)$.
% %Now, extending the previous discussion, an interval function $F:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$ is a better
% %interval representation of the function $f:U^n \rightarrow  U$ than $G:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$, denoted by $G\sqsubseteq
% %F$, if for each $\vec{X}\in \mathbb{I}_{[a,b]}^n$, the inclusion $F(\vec{X})\subseteq G(\vec{X})$ holds.
% Considering  $F:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$, let  $\underline{F},\overline{F}:U^n \rightarrow U$ be the functions
% respectively given by
% \begin{eqnarray}\label{eq-projection-Fu}
% \underline{F}(x_1,\ldots,x_n) & = &
% l(F([x_1,x_1],\ldots,[x_n,x_n])) \\ \overline{F}(x_1,\ldots,x_n) &
% = &
% r(F([x_1,x_1],\ldots,[x_n,x_n])).\label{eq-projection-Fo}\end{eqnarray}

% The fundamental theorem of interval arithmetic implicitly
% rests on the notion of correctness. It means that, when $F$ is an inclusion monotonic interval extension of a real function $f$,
% then $f(X_1,\ldots,X_n)\subseteq
% F(X_1,\ldots,X_n)$. Thus,  $F(X)$
% contains the range $f(X)$ and consequently,  the class of
% inclusion monotonic interval functions have the property of
% correctness. Moreover, since all arithmetic operations are inclusion
% monotonic functions, it holds that any rational interval function is also correct.
% In this paper, we consider the interval extension of three  real functions, which are studied in the following.

%%%%%%%%%%%%
\section{Interval Metrics}\label{sec:metrics}
%%%%%%%%%%%%%%%%%%

Not only interval algorithms  but also metrics for evaluating procedure's accuracy can assume concepts from Interval Mathematics \cite{Moore79} to perform interval analysis efficiently managing computation errors.

The motivation to consider numerical intervals instead of simple punctual values is linked to the capability of intervals to represent infinite punctual values. This sort of representation is very useful in SC when the accuracy of the input (or output) data is not granted. In these cases of uncertainty or inaccuracy the interval procedures should ensure that all possible punctual results belong to the interval results.

In addition, due to memory limitation, it is also common to compute round (or simply truncated) values to store the result afterwards. This heuristic may result in different values, depending on the machine's configuration where the program has been executed. The metrics being used to measure result quality are presented below.

\subsection{Euclidean Distance}
Let $\tilde{Y} = (\tilde{y})_{ij} \in \mathbb{R}^{n\times m}$ be an estimator of $Y = (y)_{ij} \in \mathbb{R}^{n\times m}$ whose  $n.m$ elements  $\tilde{y}_{ij}$ are predictions of the original values $(y)_{ij}$.
The Euclidean distance between  $Y$  and its estimator $\tilde{Y}$ is defined by the following expression:
\begin{equation}\label{eq-5}
D(\tilde{Y},Y) =  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{y}_{ij} - y_{ij})^2}.
\end{equation}

Analogously, $\tilde{\mathbb{Y}}= (\tilde{\mathbf{Y}})_{ij}$ is called an interval estimator of $\mathbb{Y}= (\mathbf{Y})_{ij}\in \mathbb{IR}^{n\times m}$ with an $nm$-dimensional matrix of interval predictions $\tilde{\mathbf{Y}}_{ij}$ of the original interval quantities $\mathbf{Y}_{ij}$. An interval extensional of Eq.(\ref{eq-5-1}) is given in the following:

\begin{equation}
\label{eq-5-1}
\mathbf{D}(\tilde{\mathbb{Y}}, \mathbb{Y}) =  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n} \left( \tilde{\mathbf{Y}}_{ij} - \mathbf{Y}_{ij} \right)^2}.
\end{equation}

\subsection{Mean Squared Error}
 The  Mean Squared Error ($MSE$) is considered as a risk function, corresponding to the expected value of the squared error loss.
It is an estimator measuring the average of the squares of the errors in SC, providing the difference between the estimator and what is estimated.

The $MSE$ allows us to compare the pixel values of our original image to our degraded (noise) image based on the amount by which the values of the original image differ from the degraded image.

Let $Y=(y)_{ij} \in \mathbb{R}^{n\times m}$ be the related matrix of true values $y_{ij}$.  The accuracy of $\tilde{Y} = (\tilde{y})_{ij}$ can be
obtained by the application of an MSE operator as the following
\begin{equation}\label{eq-6}
MSE ( \tilde{Y},Y) = \frac{1}{mn} \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{y}_{ij} - y_{ij})^2} = \frac{1}{mn} D(\tilde{Y},Y).
\end{equation}

Analogously, for all $\mathbb{Y}= (\mathbf{Y})_{ij}\in \mathbb{IR}^{n\times m}$  corresponding to the related matrix of true values $\mathbf{Y}_{ij}$,   the accuracy of the estimated values can be obtained by applying  an interval extension of the $MSE$ operator in  Eq.~(\ref{eq-5}), which is given by the following expression:
\begin{equation}
\mathbf{MSE} (\tilde{\mathbf{Y}},\mathbf{Y}) = \frac{1}{mn}  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{\mathbf{Y}}_{ij} - \mathbf{Y}_{ij})^2}= \frac{1}{mn} \mathbf{D}(\tilde{\mathbb{Y}}, \mathbb{Y}) .
\end{equation}

\subsection{Peak signal-to-noise ratio}
 
The Rate/Distortion (R/D) measures the image quality in terms of Peak Signal-to-Noise Ratio (${PSNR}$), expressing the ratio between the maximum possible power of a signal and the power of corrupting noise affecting the fidelity of its representation.

The $PSNR$ is usually given by the logarithmic decibel scale and  most commonly used to measure the quality of reconstruction of lossy compression codecs. In image compression, the signal is the original data and the noise is the error introduced by compression. 

Thus, the expression of ${PSNR}$ is most easily defined via the logarithmic decibel scale related to the $MSE$ as follows:
\begin{equation}\label{eq-8a}
PSNR (\tilde{Y},Y) = 10 \cdot \log_{10} \frac{MAX_I^2}{MSE(\tilde{Y},Y)} ,
\end{equation}
when $I$ indicates a $mn$-dimensional monochrome image associated to $Y$ and  $MAX_I$  is  the maximum possible pixel value of the image $I$.

Therefore, a natural interval extension of Eq.~(\ref{eq-8a}) is given as:
\begin{equation}\label{eq-8b}
\textbf{PSNR} (\tilde{\mathbf{Y}},\mathbf{Y}) = 10 \cdot \log_{10} \frac{[MAX_I,MAX_I]^2}{\mathbf{MSE}(\tilde{\mathbf{Y}}, \mathbf{Y})},
\end{equation}
when $[MAX_I,MAX_I]$ denotes the degenerate interval obtained by $MAX_I$ and $\mathbf{MSE}(\tilde{\mathbf{Y}}, \mathbf{Y})$ is the interval extension of $MSE(\tilde{Y},Y)$.

Finally, it should be noticed that a computation of the MSE between two identical images, the value will be zero and hence the $PSNR$ will be undefined. Moreover, as the main limitation, both metrics relies strictly on numerical comparison, on which  exactly focuses our study in this paper.

\section{C-XSC Library} \label{sec:intext}

To make interval results able to contain all possible punctual results, algorithms performing only interval arithmetic have to be designed. In this context, the HWT and its many formulations are extended according to the interval arithmetic, considering the C-XSC\footnote{C-XSC library can be accessed at: http://www2.math.uni-wuppertal.de/$\sim$xsc/xsc/cxsc.html} interval library.

C-XSC is an extensive C++ class library  for SC including a set of basic data types (from intervals to multiple precision complex intervals) whose   high
accuracy computation must be available for some basic arithmetic operations,
mainly the operations that accomplish the summation and (optimize) dot
product.

Therefore, some concepts of Mathematics of Computation and
Computational Arithmetic have been incorporated into this system,
such as high accuracy arithmetic, interval mathematics and  automatic numerical verification.

The compatibility between C++ and C-XSC library allows high level programming techniques for numerical applications~\cite{holbig05a}, aggregating other available libraries to support SC and also including additional packages in the source code~\cite{cxsc3}.

Besides,  C-XSC  is an open source library adding  additional packages in the source code~\cite{cxsc3}.

Function and operator overloading allow the common mathematical notation of expressions involving interval types. This is also true for (interval) matrix/vector expressions.
Numerical verification methods, also called self-validating methods, are
constructive and they allow to handle uncertain data with mathematical rigor.

The C-XSC library provides support for users to develop efficient
numerical verification methods to obtain  correct and self-validating numerical applications.


In the last years significant improvements have been made in parallelized versions of interval linear system solvers supplied by C-XSC, for more details see~\cite{kolbergfernandesclaudio2008, milani2010}. Once the interval extensions allow different error computations, different compression algorithms can be derived, as shown in Section~\ref{sec:compress}.


% With the summarized background already presented, in the remainder of %the current study the extension of the Haar wavelet transform for the %interval context is obtained. Taking advantage of the interval %arithmetics some formulation simplifications are made in order to %improve the quality of the interval extensions of the transforms. As an application, image compression is also pointed out in the interval context.

\chapter{Discrete Wavelet Transforms}\label{sec:DWTs}

This Chapter introduces both Haar Wavelet Transform (HWT), presented in Section~\ref{sec:HWT}, and Daubechies Wavelet Transform (DaWT), Section~\ref{sec:DaWT}, and their interval formulations.

\section{Haar Wavelet Transform}\label{sec:HWT}

This Section describes both decimated and undecimated formulations of the HWT, presented in Sections~\ref{sec:hwt-decimated} and \ref{sec:hwt-undecimated} respectively.

\subsection{Decimated Formulation}\label{sec:hwt-decimated}

The Haar basis was proposed in 1910, introduced by the Hungarian mathematician Alfred Haar \cite{haara} in a different approach than the one considered by Daubechies in 1988, when she presented her orthogonal basis with compact support for the space of square integrable functions\cite{daubechies1992}, called wavelets.

Nevertheless, through her work, the Haar functions started to be seen as a particular case of orthogonal wavelets. Nowadays, the HWT is well stated with some different fast algorithm formulations, been well established for many applications, specially those involving data compression, as considered by JPEG-2000 \cite{JPEG2000}.

\subsubsection{One Dimensional}\label{sec:hwt1d_decimated}

According to the description in \cite{stollnitz1995}, when assuming the decimated formulation of the transform and given an initial vector $C$ with $n$ punctual values, the one-dimensional HWT calculates the averages and differences of each pair of adjacent elements in the input vector $ C $, $(C_{2j-1}, C_{2j}), j=1,...,n/2$, generating therefore two output sets:  one for the scaling (averages) and another for the wavelets (differences) coefficients, both with $n_1= n/2$ points, half size of the original input vector C.

Figure~\ref{fig:DandDS} presents the  $log_2(n)$ level decomposition of the Int-HWT algorithm,  formulated for the decimated case as an extension of the original algorithm presented in~\cite{stollnitz1995}.
The novelty here on the interval code remains in the definition of vectors as interval vectors. Constants are defined as interval quantities and all arithmetics are treated as interval operations. 

In this sense, in Figure~\ref{fig:DandDS} the interval quantities  $h=( 1/\sqrt{[2;2]}, 1/\sqrt{[2;2]})$ and $g=( 1/\sqrt{[2;2]}, -1/\sqrt{[2;2]})$ are the  normalized interval filters associated to the Int-HWT, representing the interval extension of the standard algorithm. The discrete convolution from $C$ with $h$ generates the averages (scaling coefficients) and  for computing the differences (wavelet coefficients) the filter $g$ is considered.

Since the HWT main feature is to decompose information in many resolution levels, assuming as input the set of the scaling coefficients that contains the $n_1$ previously computed averages, the next level of the decimated HWT produces a second pair of half-sized vectors, one for the new $n_2=n_1/2$ averages and other for the corresponding $n_2=n_1/2$ wavelet coefficients.

This procedure is recursively defined and can be applied until a specific level of coarser resolution is achieved or until a single scalar scaling coefficient is obtained. The whole process of the direct HWT is also called \textbf{decomposition}.

\begin{figure}[ht]
	\centering
    \includegraphics[width=14cm]{Imagens/D_and_DStep.png}
    \caption{1D decimated Int-HWT using $h$ and $g$ filters.}
    \label{fig:DandDS}
 \end{figure}

Figure \ref{fig:ComparisonIntPonc} shows the comparison between the standard and the interval procedures of the one-dimensional HWT assuming $l=log_2 n$ decomposition levels.
%
\begin{figure}[ht]
    %\centering
    \includegraphics[width=13cm]{Imagens/FiguraA.png}
    \caption{1D decimated HWT algorithms:(top panel) original; (bottom panel) interval extension. Both cases assuming initial data as being punctual values and $log_2 n$ decomposition levels.}
    \label{fig:ComparisonIntPonc}
\end{figure}
%
It is possible to notice in Figure~\ref{fig:ComparisonIntPonc} the care with the type of data being used in the formal parameters in both procedures.

The constants must be expressed by intervals too. The instruction $sqrt(float(n))$, square root of punctual floating value, becomes $sqrt(interval(n))$, square root of punctual interval. Inside the while loop the interval procedure calls another interval procedure, maintaining the compatibility of the data being calculated.

The same type of code adaptation is considered for the rest of the punctual procedures, generating in this way the proposed interval extension.

Using the sets of all wavelet coefficients, resulted from the decomposition process, together with the averages in the lowest level of the direct decimated Int-HWT, it is possible to accurately perform the \textbf{inverse transformation} (also called \textbf{composition}), generating the exact reconstruction of the original vector, which is an important characteristic of the HWT.

The pseudo-code that performs the level composition of  the 1D decimated Int-HWT is represented in Figure~\ref{fig:CandCS}. According to \cite{stollnitz1995}, in the composition process, starting from the coarsest resolution level of the transformation, the original values are restored level by level, combining the wavelet and scaling coefficients from the level immediately below.

Therefore, the decomposition process is completely reversible, allowing the data reconstruction in each level of the transformation, until the end of the process is achieved, when finally the finest resolution level is exactly reconstructed.

\begin{figure}[ht]
    \centering
    \includegraphics[width=15cm]{Imagens/C_and_CStep.png}
    \caption{Inverse 1D decimated Int-HWT. Composition  process to reconstruct $l=log_2 n$ levels.}
    \label{fig:CandCS}
 \end{figure}

\subsubsection{Two Dimensional}
\label{sec:hwt2d}

The HWT can be extended for two-dimensional vectors. According to the procedure described in \cite{stollnitz1995}, the fast algorithm for the 2D decimated HWT is obtained through the application of the one-dimensional transformation per direction (in all rows of the input matrix and after that, in all columns of the resulting one).

\newpage

In fact, the order how the many levels of the one-dimensional transformation are applied to the two-dimensional data generates different intermediate results and, therefore, distinct algorithms for the 2D transform.

Following what was presented in \cite{stollnitz1995},  the 2D  decimated Int-HWT can be calculated through the standard method, showed in Figure~\ref{fig:SDandND}. In this formulation, the many levels of the one-dimensional transform are applied to all rows.
After the $l=log_2 n$ decomposition levels of the 1D HWT were applied to the entire set of rows of the matrix, the same 1D procedure is applied to the columns to the resulting matrix, as many levels as done for the rows.

This completes the 2D decimated transformation, assuming $l=log_2 n$ decomposition levels. Figure \ref{fig:DecompositionMethods} illustrates the application of this algorithm.

\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{Imagens/SD_and_ND.png}
    \caption{2D decimated HWT: Standard and non-standard decomposition processes.}
    \label{fig:SDandND}
 \end{figure}

 \begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{Imagens/Figura2.png}
    \caption{2D decimated HWT with $n\geq 3$ decomposition levels.(left panel)standard algorithm; (right panel) non-standard method.}
    \label{fig:DecompositionMethods}
 \end{figure}

 According to \cite{stollnitz1995} and shown in Figure \ref{fig:SDandND},the non-standard method is another option for the 2D decimated HWT, whose interval extension is also proposed here. In this formulation the operations by rows and columns through out each level of the transform are intercalated.

Thereby, every row is decomposed in one level and right after that all columns are also decomposed in one level, completing one level of decomposition for the entire 2D data. In this non standard formulation of the 2D HWT, the resulting scaling coefficients after one decomposition level, which are one fourth of the entire initial data, are then considered as the input for the computation of the next decomposition level of the transform.

The three remaining blocks with wavelet coefficients are not further decomposed. Each one also containing one fourth of the original data.  The complete process repeats itself until in the last level just a single scaling coefficient and just one wavelet coefficient for the remaining three sets are obtained. A representation of this formulation is presented in Figure~\ref{fig:DecompositionMethods}(right panel).

  \begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{Imagens/Inverse_SC_NSC.png}
    \caption{2D decimated HWT - composition process: (left panel) standard formulation; (right panel) non-standard formulation.}
    \label{fig:InverseSCandNSC}
 \end{figure}

  The direct and inverse Int-HWT, Figures \ref{fig:DandDS}, \ref{fig:CandCS}, \ref{fig:SDandND} and \ref{fig:InverseSCandNSC}, can be thought as a convolution of the input data with a pair of filters , $h=(1/2, 1/2)$ and $g=(1/2,-1/2)$, that represent the transform \cite{stollnitz1995}.  The normalized Haar filters are $h=(1/\sqrt{2}, 1/\sqrt{2})$ and $g=(1/\sqrt{2},-1/\sqrt{2})$ and the normalized transform preserves the energy of the entire set of coefficients obtained through the HWT.

\newpage

Therefore, according to \cite{stollnitz1995}, besides the standard and non-standard algorithms, the HWT can be performed using  normalized and non-normalized filters.

Thus combining the two possible algorithms with these two possible filter choices, there are four ways to compute the decimated 2D Int-HWT available: (i) non-standard, non-normalized; (ii) non-standard, normalized; (iii) standard, non-normalized; and (iv) standard, normalized.

\subsection{Undecimated Formulation}
\label{sec:hwt-undecimated}

Another alternative version for the HWT is the undecimated approach, which avoids the decimation operation after the convolution with the filters has been computed.

This undecimated transform has one well established formulation, called the {\it À-Trous} algorithm, which is considered in many astrophysical and statistical applications \cite{IEEE2007,kozakevicius2013}, specially for been a translation invariant transform.

\subsubsection{One Dimensional}
\label{sec:hwt1d-undecimated}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
According to what is shown in \cite{IEEE2007}, given an initial vector $ C^0 $ with $n_0=2^J$ punctual values, the vector $C^1$ that contains the first decomposition level of the 1D undecimated HWT (À-Trous HWT) stores the averages for each pair of adjacent elements $(C^0_j,C^0_{j+1}), j=1,...,n_0$, (assuming $C^0_{n_0+1}=C^0_{0}$).

The difference now with respect to the Cascade formulation is that the range of $j$ to select the values $C^0_j$ covers all positions $j=1,...,n_0$. This is exactly what avoids the decimation step and causes $C^1$ to have the same size as $C^0$. The way the wavelet coefficients are computed  also differs from the previous formulation. Now the vector with the differences $D^1$  is computed by $D^1_j=C^0_j -C^1_j, \ j=1,...,n_0$.

The  {\it À-Trous} HWT is again constructed recursively, until a certain decomposition level $0<l<log_2 n_0$ is achieved. In this sense, the vector of averages $C^j$  at a level $0< j \leq l$ is obtained from the set $C^{j-1}$, the averages at the previous level. Analogously, the vector with the wavelet coefficients is given by $D^j=C^{j-1}-C^j$.  This process is  again called Decomposition or Direct Transformation and it is illustrated in Figure~\ref{fig:DTAtrous} .

%The operations over the last element of the vector are %performed using the first element from the same vector %to form its pair, avoiding the necessity of %extrapolation to deduce the next element after the end %of a vector.

\begin{figure}[ht]
	\vspace{-0.2cm}
    \normalsize
      \begin{tabular}{l l l}
          $C^{0} = [9 \quad 7 \quad 5 \quad 3] \quad \rightarrow $ & $ C^1 = [8 \quad 6 \quad 4 \quad 6] \quad \rightarrow $ & $ C^{2} =  [7 \quad 5 \quad 5 \quad 7] \quad \rightarrow \enskip ... $  \\
          $ $ & $ D^1 = [1 \quad 1 \quad 1 -3] $ & $ D^{2} =  [1 \quad 1 -1 -1] $
      \end{tabular}
      \caption{1D À-Trous HWT: example with 2 decomposition levels}
    \label{fig:DTAtrous}
    \vspace{-0.3cm}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=9cm]{Imagens/Atrous_D_and_DStep.png}
	\caption{1D À-Trous HWT- Decomposition algorithm.}
    \label{fig:AtrousDecompMethods}
\end{figure}

Summing all vectors of wavelet coefficients resulted by the $n $ decomposition levels together with the averages in the lowest level $n$, it is possible to perform, accurately, the Inverse Transformation,  $ C^0 = C^{n} + \sum_{i=1}^{n} D^{i} $,
also called Composition, which provides the exact reconstruction of the original input vector of data, being an important characteristic of Wavelet Transforms. The composition pseudo code is shown in Figure~\ref{fig:AtrousCompMethod}.

\begin{figure}[ht]
\includegraphics[width=7cm]{Imagens/Atrous_C.png}
	\caption{1D À-Trous HWT- Composition algorithm.}
    \label{fig:AtrousCompMethod}
\end{figure}

\subsubsection{Two Dimensional}
\label{sec:hwt2d-undecimated}

According to \cite{IEEE2007}, the algorithm for the 2D À-Trous HWT is again obtained through the application of the one-dimensional transformation in all rows and columns of the input matrix, as done for the decimated version of the HWT.

Analogously, Standard and Non-Standard procedures can be designed according to the order how the 1D À-Trous transforms are applied to the 2D data.

\begin{figure}[h!]
\includegraphics[width=15cm]{Imagens/HaarAtrousStandard.png}
	\caption{2D À-Trous HWT- standard algorithm for matrix decomposition.}
    \label{fig:AtrousStandard}
\end{figure}

\begin{figure}[h!]

\includegraphics[width=10cm]{Imagens/HaarAtrousStandardPseudo.png}

	\caption{2D À-Trous HWT: Pseudo code for the standard matrix decomposition.}
    \label{fig:AtrousStandardPseudo}

\end{figure}

The undecimated standard decomposition for the 1D Int-HWT, based on the Standard decimated decomposition, is performed by executing the 1D À-Trous Int-HWT for each row of the input matrix. This process is called Row~À-Trous~Transform~(\textit{RAT}), characterizing this procedure as a horizontal transformation.

Considering $n$ decomposition levels, the RAT procedure is repeated $n$ times, generating matrices $ C^1_R $ ... $ C^n_R $ and $ D^1_R $ ... $ D^n_R $. After decomposing all rows from the input matrix, the decomposition of all its columns is performed, called Column~À-Trous~Transform~(\textit{CAT}), characterizing a vertical transformation and generating matrices $ C^{n+1}_C $ ... $ C^{n*2}_C $ and $ D^{n+1}_C $ ... $ D^{n*2}_C $.

The matrices $ C^j_R $ and $ C^j_C $  store scaling coefficients, while $ D^j_R $ and $ D^j_C $ store wavelet coefficients. A visual representation of the process is shown in Figure~\ref{fig:AtrousStandard}, and its pseudo algorithm is shown in Figure~\ref{fig:AtrousStandardPseudo}.

\begin{figure}[h!]

	\includegraphics[width=11cm]{Imagens/HaarAtrousNonStandardPseudo.png}

	\caption{Pseudo code for the Non Standard matrix decomposition.}
    \label{fig:AtrousNonStandardPseudo}

\end{figure}

\begin{figure}[h!]

	\includegraphics[width=15cm]{Imagens/HaarAtrousNonStandard.png}

	\caption{Non Standard algorithm for matrix decomposition.}
    \label{fig:AtrousNonStandard}

\end{figure}

The undecimated non-standard decomposition, based on the original non-standard algorithm from the Cascade approach, is performed by intercalating both \textit{RAT} and \textit{CAT} operations, extracting vertical and horizontal details for each level of decomposition, as shown in Figure~\ref{fig:AtrousNonStandard}.

For each level $ j $ of decomposition the algorithm performs a \textit{RAT} step, generating matrices $ C^j_R $ and $ D^j_R $, and right after a \textit{CAT} is performed, generating matrices $ C^j_C $ and $ D^j_C $.

Both $ C^j_R $ and $ C^j_C $ store scalar coefficients, while $ D^j_R $ and $ D^j_C $ hold wavelet coefficients, and those four matrices represent one level full decomposed. The À-Trous Non Standard algorithm is shown in Figure~\ref{fig:AtrousStandardPseudo}.

The same idea from the one-dimensional transform can be applied for both standard and non-standard sets of data to perform an inverse process, generating the input matrix.

\newpage

By using all matrices of wavelet coefficients resulted by the process of decomposition together with the averages in the lowest level of decomposition, it is possible to reconstruct, accurately. $MatrixComposition$ generates the exact reconstruction of the original input matrix of data. The algorithm describing its execution is shown in Figure~\ref{fig:ATrousMatrixComposition}.

\begin{figure}[h!]
	\includegraphics[width=11cm]{Imagens/ATrousMatrixComposition.png}
	\caption{Pseudo code for À-Trous matrix composition.}
    \label{fig:ATrousMatrixComposition}

\end{figure}

\newpage

\section{Daubechies Wavelet Transform}\label{sec:DaWT}

This Section introduces the Daubechies Wavelet Transform (DaWT) as described in the literature~\cite{DAUBTHESIS}, in addition to the proposed interval filters. The one and two transformation methods are presented in Subsections~\ref{sec:1DDaWT} and \ref{sec:2DDaWT} respectively.

\subsection{One Dimensional}\label{sec:1DDaWT}

According to the description in \cite{DAUBTHESIS}, when assuming the original formulation of the transform and given an initial vector $C$ with $n$ punctual values, the one-dimensional DaWT calculates scaling and wavelet coefficients of each pair of adjacent elements in the input vector $C$, $(C_{2j-1}, C_{2j}), j=1,...,n/2$, generating therefore two output sets: a vector containing the scaling and a second vector with wavelet coefficients, both with $n_1= n/2$ points, half size of the original input vector $C$.

\begin{figure}[h!]
    \includegraphics[width=7.2cm]{Imagens/Daub_D.png}
    
    \includegraphics[width=14cm]{Imagens/Daub_DStep.png}
    \caption{1D DaWT decomposition procedure.}
    \label{fig:DaubDandDS}
\end{figure}

\newpage

Figure~\ref{fig:DaubDandDS} presents the $log_2(n)$ level decomposition of the Int-DaWT algorithm, formulated as an extension of the original algorithm presented in~\cite{DAUBTHESIS}.

Constants are defined as interval quantities and all arithmetics are treated as interval operations. In this sense the interval extension of the DaWT is performed by using the set of filters $h'$ and $g'$, shown in Eqs.~(\ref{eq:int-hfilters})~and~(\ref{eq:int-gfilters}), based on the original set of filters $h$ and $g$, shown in Eqs.~(\ref{eq:hfilters})~and~(\ref{eq:gfilters}), which are normalized filters associated to the Int-DaWT.

The discrete convolution from $C$ with $h$ generates the next set of scaling coefficients and for computing the wavelet coefficients the filter $g$ is considered.

%\begin{figure}[h!]
%    \includegraphics[width=14cm]{Imagens/Daub_DStep.png}
%    \caption{1D DaWT decomposition procedure.}
%    \label{fig:DaubDS}
% \end{figure}

\setcounter{equation}{0}

\begin{eqnarray}
%\Large
h' & = & (
\frac{[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[3;3]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
)\label{eq:int-hfilters}\\
g' & = & (
\frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{-[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{-[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
)\label{eq:int-gfilters}\\
ih' & = & (
\frac{[3;3]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
)\label{eq:int-ihfilters}\\
ig' & = & (
\frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{-[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\enskip \frac{-[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
)\label{eq:int-igfilters}
\end{eqnarray}

Since the DaWT main feature is to decompose information in many resolution levels, assuming as input the set of the scaling coefficients that contains the $n_1$ previously computed averages.

The next decomposition level produces a second pair of half-sized vectors, one for the new $n_2=n_1/2$ scaling coefficients and other for the corresponding $n_2=n_1/2$ wavelet coefficients. This procedure is recursively defined and can be applied until a specific level of coarser resolution is achieved or until two scaling coefficients are obtained.

\begin{eqnarray}
%\Large
h & = & (
\quad \frac{1+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3-\sqrt{3}}{4\sqrt{2}},
\quad \frac{1-\sqrt{3}}{4\sqrt{2}}
\quad )\label{eq:hfilters}\\
g & = & (
\quad \frac{1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{-3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1-\sqrt{3}}{4\sqrt{2}}
\quad )\label{eq:gfilters}\\
ih & = & (
\quad \frac{3-\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{1+\sqrt{3}}{4\sqrt{2}},
\quad \frac{1-\sqrt{3}}{4\sqrt{2}}
\quad )\label{eq:ihfilters}\\
ig & = & (
\quad \frac{1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1+\sqrt{3}}{4\sqrt{2}}
\quad )\label{eq:igfilters}
\end{eqnarray}

Using the sets of all wavelet coefficients, resulted from the decomposition process, together with the scaling coefficients in the lowest level of the decomposed data, it is possible to perform the inverse transformation, also known as composition, reconstructing the original vector, which is an important characteristic of DWTs.

\begin{figure}[h!]
    \includegraphics[width=7.1cm]{Imagens/Daub_C.png}
    
    \includegraphics[width=14cm]{Imagens/Daub_CStep.png}
	\caption{Inverse 1D DaWT, Composition process to reconstruct $l=log_2 n$ levels.}
    \label{fig:DaubCandCS}
\end{figure}

The pseudo-code that performs the level composition of the 1D DaWT is represented in Figure~\ref{fig:DaubCandCS}. The filter sets $ih'$ and $ig'$, presented in Eqs.~(\ref{eq:int-ihfilters})~and~(\ref{eq:int-igfilters}), are based on the original filter sets $ih$ and $ig$, described in Eqs.~(\ref{eq:ihfilters})~and~(\ref{eq:igfilters}).

According to \cite{DAUBTHESIS}, in the composition process, starting from the coarsest resolution level of the transformation, the original values are restored level by level, combining the wavelet and scaling coefficients, calculated by using filter sets $ih'$ and $ig'$, from the level immediately below.

Therefore, the decomposition process is completely reversible, allowing the data reconstruction in each level of the transformation, until the end of the process is achieved, when finally the finest resolution level is exactly reconstructed.

%\begin{figure}[h!]
%    \includegraphics[width=14cm]{Imagens/Daub_CStep.png}
%    \caption{Inverse 1D DaWT, Step Composition process.}
%    \label{fig:DaubCS}
% \end{figure}

\subsection{Two Dimensional}\label{sec:2DDaWT}

The DaWT can be extended for two-dimensional vectors by using the same concept presented for the HWT in Section~\ref{sec:hwt2d}. According to the procedure described in \cite{DAUBTHESIS}, the fast algorithm for the 2D DaWT is obtained through the application of the one-dimensional transformation per direction (in all rows of the input matrix and after that, in all columns of the resulting one).

In fact, the order how the many levels of the one-dimensional transformation are applied to the two-dimensional data generates different intermediate results and, therefore, distinct algorithms for the 2D transform.

Following what was presented in \cite{DAUBTHESIS}, the 2D DaWT can be calculated through the standard method, showed in Figure~\ref{fig:SDandND}. In this formulation, the many levels of the one-dimensional transform are applied to all rows.

After the $l=log_2 n$ decomposition levels of the 1D DaWT were applied to the entire set of rows of the matrix, the same 1D procedure is applied to the columns of the resulting matrix, as many levels as done for the rows. This completes the 2D transformation, assuming $l=log_2 n$ decomposition levels. Figure \ref{fig:DecompositionMethods} illustrates the application of this algorithm.

According to \cite{DAUBTHESIS} and shown in Figure \ref{fig:SDandND},the non-standard method is another option for the 2D DaWT. In this formulation the operations by rows and columns through out each level of the transform are intercalated.

Thereby, every row is decomposed in one level and right after that all columns are also decomposed in one level, completing one level of decomposition for the entire 2D data. In this non standard formulation of the 2D DaWT, the resulting scaling coefficients after one decomposition level, which are one fourth of the entire initial data, are then considered as the input for the computation of the next decomposition level of the transform.

The three remaining blocks with wavelet coefficients are not further decomposed. Each one also containing one fourth of the original data.  The complete process repeats itself until in the last level just a single scaling coefficient and just one wavelet coefficient for the remaining three sets are obtained. A representation of this formulation is presented in Figure~\ref{fig:DecompositionMethods}(right panel).

The direct and inverse of the DaWT, Figures \ref{fig:DaubDandDS}, \ref{fig:DaubCandCS}, \ref{fig:SDandND} and \ref{fig:InverseSCandNSC}, can be thought as a convolution of the input data with two normalized pairs of filters, $h$ and $g$ shown in Eqs.~(\ref{eq:hfilters}) and (\ref{eq:gfilters}), that represent the transform.

The normalized transform preserves the energy of the entire set of coefficients obtained through the process. Therefore, by using the same strategy implemented for the HWT, besides the standard and non-standard algorithms, the DaWT can be performed using  normalized and non-normalized filters.

Thus combining the two possible algorithms with these two possible filter choices, there are four ways to compute the decimated 2D DaWT available: (i) non-standard, non-normalized; (ii) non-standard, normalized; (iii) standard, non-normalized; and (iv) standard, normalized. The non-normalized filters are presented in Section~\ref{sec:Daub1DOptimization}.

\chapter{Developed Optimal Formulations}
\label{sec:optintform}

When the normalized filters are considered in the original algorithms of the HWT and DaWT \cite{stollnitz1995, IEEE2007, DAUBTHESIS}, divisions by  $ \sqrt{2} $  are executed in all iterations within the decomposition.

Once  $ \sqrt{2} $ is not a computable value, each level of the decomposition or composition adds a certain degree of error into the data. The error generated in each iteration is then propagated through all levels until the end of the procedure.

The solution proposed to avoid this issue by performing algebraic simplifications in order to eliminate the computation of these non computable values, $2^{j/2}$ , whenever possible, reducing the error involved in the process.

Therefore the developed algorithms produce more trustworthy results in comparison to the literature.

The basic idea for the algebraic simplifications is based on using the non-normalized transform, which introduces no computing errors for the HWT.

By shifting the normalization step to the end of the transform, all operations with $ \sqrt{2} $ are applied only once, decreasing the error produced by its computation in each level of the decomposition or composition.

\section{Haar Wavelet Transform}

This Section aims to describe the developed optimizations for the HWT where the optimized decimated formulation is presented in Section~\ref{sec:hwt-dec} and its undecimated form in Section~\ref{sec:hwt-undec}.

\subsection{Decimated Formulation}\label{sec:hwt-dec}

This Section presents the developed optimizations for both one and two dimensional approaches of the HWT in Sections~\ref{subsec:1DCascadeOpt} and \ref{subsec:cascade2D_Opt_Haar}.

\subsubsection{One Dimensional}\label{subsec:1DCascadeOpt}

 The normalized decomposition procedure for the decimated version of the Int-HWT, either 1D or 2D, is executed in the following order: first a non-normalized decomposition is made, as indicated by the pseudo-code in Figure~\ref{fig:Fig4}.

After this stage the normalization of all coefficients is performed, multiplying all of them by the normalization factor $ 2^{-j/2} $, where $ j $ is the resolution level of the coefficients through out the decomposition.

Depending on $ j $, the computation of $ \sqrt{2} $ may not be necessary, avoiding  any computation error in this case. This process is illustrated in Figure \ref{fig:Fig4}, assuming 2 levels of the 1D HWT and considering the same example as presented in \cite{stollnitz1995}.

 \begin{figure}[ht]
 	\large
    \centering
    $[9 \quad 7 \quad 3 \quad 5] \rightarrow [6 \quad 2 \quad 1 \quad -1] \rightarrow [6 \quad 2 \quad \frac{1}{\sqrt{2}} \quad \frac{-1}{\sqrt{2}}]$

    \vspace{0.2cm}
	\normalsize
    $ \textbf{Input} \quad \quad \rightarrow \textbf{Decomposition} \rightarrow \textbf{Normalization} $
    \caption{Example of the one-dimensional optimization.}
    \label{fig:Fig4}
 \end{figure}

Looking at the original decomposition procedure, shown in Figure~\ref{fig:DandDS}, the complexity of $DecompositionStep$ is $\BigO(n + 1)$, and for the main $ decomposition $ algorithm the complexity is $\BigO(n + \log(n) * (n + 2) + 1)$. By the proposed simplification they are reduced to $\BigO(n)$ and $\BigO(n + n\log(n))$, respectively. By performing the non-normalized decomposition there is no need to execute the normalization step at line 1 of the original $decomposition$ algorithm, neither the divisions by $\sqrt{2}$ at its step procedure. Therefore, overall complexity for non-normalized transformation of a $n$ sized vector is $\BigO(n\log(n))$, representing the pseudo-code shown in Figure~\ref{fig:NewDandDS}.

\begin{figure}[ht]
    %\centering
    \includegraphics[width=13.6cm]{Imagens/New_D_and_DStep.png}
    \caption{Int-HWT, decimated version: non-normalized Decomposition process and its step procedure.}
    \label{fig:NewDandDS}
 \end{figure}

The normalization procedure, presented in Figure~\ref{fig:1DNorm_and_Denorm}, performs the normalization step of all coefficients after their convolution using filters. Since it is executed once on every coefficient of a $n$ sized vector, its complexity can be $\BigO(n)$. 

By performing a non-normalized transform and the normalization procedure afterwards, the complexity is $\BigO(n\log(n) + n)$, which is the same complexity from the normalized transformation.

The goal is therefore the gain on accuracy obtained by avoiding high number of divisions using $\sqrt{2}$ from the original $DecompositionStep$ algorithm, as shown in Figure~\ref{fig:test1-3}.

\begin{figure}[ht]
	%\centering
    \includegraphics[width=12.4cm]{Imagens/Norm_and_Denorm.png}
    \caption{Int-HWT, decimated version: on dimensional normalization procedure.}
    \label{fig:1DNorm_and_Denorm}
 \end{figure}

The normalized composition procedure follows the same basic idea to reduce computation of $\sqrt{2}$, it performs the non-normalized inverse transform, but this time a denormalization step, shown in Figure~\ref{fig:1DNorm_and_Denorm}, is executed before the transform begins. 

The original algorithms for composition and its step procedure, Figure~\ref{fig:CandCS}, behave similarly to decomposition. The complexity of the $Composition$ procedure is $\BigO(n\log(n) + n)$ if the normalized formulation is considered, otherwise it is $\BigO(n\log(n))$. The denormalization step, also shown in Figure~\ref{fig:1DNorm_and_Denorm}, is almost the same $Normalization$ algorithm, whose complexity is $\BigO(n)$.

The difference is at line 6, where it is dividing each coefficient by the normalization factor, preparing the data to be transformed. Therefore the optimal composition has the complexity from the original version, $\BigO(n\log(n) + n)$, but its results are more exact due to the normalization step, avoiding the calculation of $\sqrt{2}$ on every iteration.



\subsubsection{Two Dimensional}\label{subsec:cascade2D_Opt_Haar}
%
The simplifications implemented for the 2D int-HWT consider the same principle presented in the 1D case, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{-j/2}$ normalization factor, where $j$ is the corresponding level. To compose or decompose a matrix the 1D transform is applied in all rows and in all columns of the input matrix.

However, according to \cite{stollnitz1995}, there are two main algorithms for composition and decomposition of matrices, know as Standard and Non-Standard procedures, what suggests distinct normalization procedures for both approaches.

The study of the original algorithms~\cite{stollnitz1995} helped to recognize patterns of normalization factors. These patterns were analysed and employed in the development of the normalization procedures for both algorithms, assuming the decimated version of the HWT.

These patterns are illustrated in Figure~\ref{fig:2DNormFactors}, where 3 decomposition levels are presented for $8$x$8$ matrices. The parameters $ j' $ and $ j'' $ indicate the  normalization levels. The rule to calculate these normalization factors is expressed by $R$ defined in Eq.~(\ref{eq:NormFac}):

\begin{equation}
	\large
	R = 2^{\frac{-(j' + j'')}{2}} \label{eq:NormFac}
\end{equation}

where $ 0 \leq j' , j'' \leq (\log_{2}n) -1 $ and $ n $ indicates the matrix order.

\begin{figure}[ht]
  	\centering
  	\includegraphics[width=13.6cm]{Imagens/NormFactors.png}
  	\caption{Int-HWT, decimated version: (a) standard, (b) non-standard normalization patterns and (c) rule for normalization factors.}
	\label{fig:2DNormFactors}
\end{figure}

By analyzing the original standard algorithm, Figure~\ref{fig:SDandND}, it depends on the original normalized decomposition procedure, whose complexity is $\BigO(n\log(n) + n)$, executing it for every row and column of the input matrix.

Considering a $n$x$n$ matrix the complexity of the original standard algorithm is $\BigO(n(n\log(n) + n) + n(n\log(n) + n))$, therefore $\BigO(2n^2\log(n) + 2n^2)$. Performing the non-normalized decomposition algorithm, discussed in Section~\ref{subsec:1DCascadeOpt}, it reduces the calculations by avoiding intermediate normalization steps.

\begin{figure}[ht]
  	\includegraphics[width=10cm]{Imagens/StandardNormalization.png}
  	\caption{2D decimated Int-HWT: Normalization step for the standard decomposition process.}
	\label{fig:StandardNormAlg}
\end{figure}

The non-normalized standard decomposition is performed with $\BigO(2n^2\log(n))$, and the last step is to normalize the results using the $StandardNormalization$ algorithm, presented in Figure~\ref{fig:StandardNormAlg}, and since it is operating one time on each coefficient of the input matrix its complexity can be expressed by $\BigO(n^2)$.

The complexity of performing the non-normalized standard decomposition and the standard normalization step can be expressed by $\BigO(2n^2\log(n) + n^2)$, which is faster than the original normalized procedure by avoiding a second iteration of $\BigO(n^2)$ on the input matrix.

The original normalized standard composition, presented in Figure~\ref{fig:InverseSCandNSC}, can be analyzed in the same way as its decomposition procedure since it performs the same number of operations but produces an approximation of the input matrix used for decomposition. The original version is executed in $\BigO(2n^2\log(n) + 2n^2)$ time, while its optimized version operates in $\BigO(2n^2\log(n) + n^2)$.

Looking at the original non-standard algorithm, Figure~\ref{fig:SDandND}, it is executing a intermediate normalization at line 1, dividing each coefficient by the order of the input matrix, which configures a $\BigO(n^2)$ operation.

Considering a quadratic matrix of order $n$, the rest of the algorithm operates in a $while$ loop which is executed $\log_2(n)$ times. For each $while$ loop there are two $for$ loops also executing $\log_2(n)$ times, each one performing the $DecompositionStep$ algorithm ($\BigO(n)$) on portions of the input matrix, depending on the level of transformation. The original non-standard decomposition is executed in $\BigO(n^2 + \log(\log(n) + \log(n)))$ time.

\begin{figure}[ht]
  	\includegraphics[width=8.6cm]{Imagens/NonStandardNormalization.png}
  	\caption{Int-HWT, decimated version: Normalization step for the non-standard decomposition process.}
	\label{fig:NonStandardNormAlg}
\end{figure}

The non-normalized non-standard decomposition ( $\BigO(\log(\log(n) + \log(n)))$) does not need the normalization step,  and its $Non-StandardNormalization$ algorithm, shown in Figure~\ref{fig:NonStandardNormAlg}, is performed with complexity $\BigO(n^2)$.

The overall complexity of these procedures is $\BigO(n^2 + \log(\log(n) + \log(n)))$, which is the same from the original normalized non-standard algorithm. The gain of time presented in Figure~\ref{fig:test2S-NS} was obtained performing simple divisions by $2$ which are faster than using $\sqrt{2}$ as divisor when analysing processor microinstructions.

\subsection{Undecimated Formulation}\label{sec:hwt-undec}


The simplifications implemented for the 2D int-HWT consider the same principle presented in the 1D case, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{-j/2}$ normalization factor, where $j$ is the corresponding level. To compose or decompose a matrix the 1D transform is applied in all rows and in all columns of the input matrix.

However, according to what was discussed in Section~\ref{sec:hwt2d}, there are two algorithms for composition and decomposition of matrices, what suggests distinct normalization procedures for both approaches.

The developed study of the original algorithms~\cite{stollnitz1995} showed a pattern of normalization factors. This pattern was analyzed and employed in the development of the normalization procedures for both standard and non-standard algorithms, assuming the decimated version of the HWT.

These patterns are illustrated in Figure~\ref{fig:2DNormFactors}, where three decomposition levels are presented for $8$x$8$ matrices. The parameters $ j' $ and $ j'' $ indicate the  normalization levels. The rule to calculate these normalization factors is expressed by $R$ defined in Eq.~(\ref{eq:NormFac})

By analyzing the original standard algorithm, Figure~\ref{fig:SDandND}, it depends on the original normalized decomposition procedure, whose complexity is $\BigO(n\log(n) + n)$, executing it for every row and column of the input matrix.

Considering a $n$x$n$ matrix the complexity of the original standard algorithm is $\BigO(n(n\log(n) + n) + n(n\log(n) + n))$, therefore $\BigO(2n^2\log(n) + 2n^2)$. Performing the non-normalized decomposition algorithm, discussed in Section~\ref{subsec:1DCascadeOpt}, it reduces the calculations by avoiding intermediate normalization steps.

The non-normalized standard decomposition is performed with $\BigO(2n^2\log(n))$, and the last step is to normalize the results using the $StandardNormalization$ algorithm, presented in Figure~\ref{fig:StandardNormAlg}, and since it is operating one time on each coefficient of the input matrix its complexity can be expressed by $\BigO(n^2)$.

The complexity of performing the non-normalized standard decomposition and the standard normalization step can be expressed by $\BigO(2n^2\log(n) + n^2)$, which is faster than the original normalized procedure by avoiding a second iteration of $\BigO(n^2)$ on the input matrix.

The original normalized standard composition, presented in Figure~\ref{fig:InverseSCandNSC}, can be analyzed in the same way as its decomposition procedure since it performs the same number of operations but produces an approximation of the input matrix used for decomposition. The original version is executed in $\BigO(2n^2\log(n) + 2n^2)$ time, while its optimized version operates in $\BigO(2n^2\log(n) + n^2)$.

Looking at the original non-standard algorithm, Figure~\ref{fig:SDandND}, it is executing a intermediate normalization at line 1, dividing each coefficient by the order of the input matrix, which configures a $\BigO(n^2)$ operation.

Considering a quadratic matrix of order $n$, the rest of the algorithm operates in a $while$ loop which is executed $\log_2(n)$ times. For each $while$ loop there are two $for$ loops also executing $\log_2(n)$ times, each one performing the $DecompositionStep$ algorithm ($\BigO(n)$) on portions of the input matrix, depending on the level of transformation. The original non-standard decomposition is executed in $\BigO(n^2 + \log(\log(n) + \log(n)))$ time.

The non-normalized non-standard decomposition ( $\BigO(\log(\log(n) + \log(n)))$) does not need the normalization step,  and its $Non-StandardNormalization$ algorithm, shown in Figure~\ref{fig:NonStandardNormAlg}, is performed with complexity $\BigO(n^2)$.

The overall complexity of these procedures is $\BigO(n^2 + \log(\log(n) + \log(n)))$, which is the same from the original normalized non-standard algorithm. The gain of time presented in Figure~\ref{fig:test2S-NS} was obtained performing simple divisions by $2$ which is faster than using $\sqrt{2}$ as divisor.

\subsubsection{One Dimensional}

The normalized decomposition procedure for the undecimated version of the Int-HWT, either one-dimensional or two-dimensional, is executed in the same order from the optimization developed for the decimated version: first a non-normalized decomposition is made, as indicated in Figure~\ref{fig:DTAtrous}; after this stage the normalization of all coefficients is performed, multiplying all of them by the normalization factor $ 2^{j/2} $, where $ j $ is the resolution level of the coefficients through out the decomposition.

The complexities of both $ DecompositionStep $ and $ Decomposition $ procedures are  $\BigO(2n + 2)$ and $\BigO(Levels.(2n + 3))$, respectively. Both are part of the original decomposition procedure, shown in Figure~\ref{fig:AtrousDecompMethods}. As in the previous cases, by performing a non-normalized decomposition there is no need to execute divisions by $\sqrt{2}$ in the step procedure, pseudo-code shown in Figure~\ref{fig:NewAtrousDS}, reducing the corresponding complexities to $\BigO(2n)$ and $\BigO(2n.Levels)$ (the same complexity as the original normalized transform).

\begin{figure}[ht]
  	\includegraphics[width=10cm]{Imagens/Atrous_New_DStep.png}
  	\caption{2D undecimated Int-HWT: non-normalized decomposition step.}
	\label{fig:NewAtrousDS}
\end{figure}

The normalization procedure, presented in Figure~\ref{fig:Atrous1DNormalization}, performs the normalization step of all coefficients after the transform. In contrast with the decimated version, the undecimated approach deals with a set of vectors as result of the transform, and each resultant vector need to be normalized individually.

The normalization procedure multiplies each scalar coefficient by its normalization factor $ 2^{i/2} $ for every resultant vector from the transform, configuring $ \BigO(n.Levels) $ from line 1 to 5 of the pseudo-code. With the new set of scalar values all wavelet coefficients are then recalculated, configuring $ \BigO(Levels.(n + 3) + 2) $ from line 6 to 16, which can be simplified to $ \BigO(n.Levels) $. The overall complexity of the procedure is $ \BigO(2n.Levels) $.

\begin{figure}[ht]
  	\includegraphics[width=9cm]{Imagens/Atrous1DNormalization.png}
  	\caption{1D undecimated Int-HWT: normalization procedure.}
	\label{fig:Atrous1DNormalization}
\end{figure}

By performing a non-normalized vector transform and the normalization procedure, the complexity is $\BigO(4n.Levels)$, which is more expensive than the original normalized formulation. The gain obtained with the new implementation is on the accuracy of results by avoiding high number of divisions using $\sqrt{2}$ from the original $DecompositionStep$ algorithm, as shown in Figure~\ref{fig:test1-3}.

The composition process, shown in Figure~\ref{fig:AtrousCompMethod}, starts with a copy of all scalar coefficients from the lowest level summing it with all wavelet vectors produced during decomposition. The complexity of this procedure is $ \BigO(n.Levels) $ and can be used for both normalized and non-normalized transformations, there is no need for optimization.

\subsubsection{Two Dimension}

As done for the decimated case, the undecimated transform, preformed by the À-Trous algorithm, considers the same principle presented in the one-dimensional optimization, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{j/2}$ normalization factor, where $j$ is the corresponding level.

In order to decompose a data matrix the two-dimensional implementation creates a set of matrices carrying scalar coefficients and another set of matrices to store wavelet coefficients. The procedure is expensive on both performing and storing the results. Applying the same idea used in previous implementations, it is possible to reduce the error involved in the process.

In Figure~\ref{fig:AtrousStandardPseudo} the original standard procedure is presented. It performs the decomposition of every line and level of a $n$x$m$ matrix, configuring a complexity of $ \BigO(n.2m.Levels) $.

The analog procedure for all columns of the same matrix has $\BigO(m.2n.Levels)$. The entire procedure is expressed by $\BigO(n.2m.Levels + m.2n.Levels)$, which can be simplified to $\BigO(4n.m.Levels)$.

The non-standard procedure executes the same amount of calculations, but intercalating the decomposition of rows and columns. Both standard and non-standard algorithms can be used in conjunction with $DecompositionStep$ shown in Figure~\ref{fig:NewDandDS}, so the complexity of executing the normalized and non-normalized are the same.

The next step in order to normalize the transformed values is to perform the $A\-TrousMatrixNormalization$ procedure, shown in Figure~\ref{fig:AtrousMatrixNormalization}, which is executed in $\BigO(4n.m.Levels)$.

Therefore, the optimized decomposition procedure developed costs $\BigO(8n.m.Levels)$, twice the cost of the original normalized formulation, but tests indicate that our approach results in more exact values as shown in Figure~\ref{fig:test4S-NS}.

\begin{figure}[ht]
  	\includegraphics[width=11.5cm]{Imagens/ATrousMatrixNormalization.png}
  	\caption{2D undecimated Int-HWT: normalization procedure.}
	\label{fig:AtrousMatrixNormalization}
\end{figure}

\newpage

\section{Daubechies Wavelet Transform}

This Section discribes the developed optimizations for both 1D and 2D DaWT, in addition to new filter sets for calculation accuracy, presented in Subsections~\ref{sec:Daub1DOptimization} and \ref{sec:Daub2DOptimization} respectively.

\subsection{One Dimensional} \label{sec:Daub1DOptimization}

The normalized decomposition procedure for the DaWT 1D is executed in the following order: 
\begin{description}
\item (i) firstly,  a non-normalized decomposition is made, as indicated by the pseudo-code in Figure~\ref{fig:Fig4}; and, 

\item (ii) the normalization of all coefficients is performed, multiplying all of them by the normalization factor $ 2^{j/2} $, where $ j $ is the resolution level of the coefficients through out the decomposition.
\end{description}

The last one stage represents the same process used in the HWT presented in Figure~\ref{fig:Fig4}, depending on $j$, the computation of $\sqrt{2}$ may not be necessary, avoiding computing error in this case.

As presented in Figure~\ref{fig:Daub_Opt_D_and_DStep}, in the decomposition steps there are calculations involving irrational numbers.

\begin{figure}[h!]
    %\centering
    \includegraphics[width=8cm]{Imagens/Daub_Opt_D.png}
    
    \vspace{0.5cm}
    
    \includegraphics[width=12cm]{Imagens/Daub_Opt_DStep.png}
    \caption{DaWT Decomposition pseudo-code for normalized and non-normalized transformations.}
    \label{fig:Daub_Opt_D_and_DStep}
 \end{figure}
 
The next step of the optimization is to reduce the number of operations using $\sqrt{3}$ by considering the filters $h$, $g$, $ih$ and $ig$, described in the following paragraphs. 

Firstly, the algebraic manipulations  presented in Eq.(\ref{eq:hOptFilter}), express the simplifications related to parameter $h$, expressed by Eq.(\ref{eq:hfilters}), by taking  $a$, $b$, $c$, and $d$ as different  different coefficients.
\begin{eqnarray}
	h & = & a\left(\frac{1+\sqrt{3}}{4\sqrt{2}}\right) + b\left(\frac{3+\sqrt{3}}{4\sqrt{2}}\right) + c\left(\frac{3-\sqrt{3}}{4\sqrt{2}}\right) + d\left(\frac{1-\sqrt{3}}{4\sqrt{2}}\right) \nonumber \\
	 & = & \frac{a\left(1+\sqrt{3}\right) + b\left(3+\sqrt{3}\right) + c\left(3-\sqrt{3}\right) + d\left(1-\sqrt{3}\right)}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a+a\sqrt{3} + 3b+b\sqrt{3} + 3c-c\sqrt{3} + d-d\sqrt{3}}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a + d + 3\left(b + c\right) + \sqrt{3}\left(a + b - c - d\right)}{4\sqrt{2}} \label{eq:hOptFilter}
\end{eqnarray}

In the sequence, Eq.(\ref{eq:gOptFilter}) provides the main simplifications related to parameter $g$, based on previous expression in Eq.(\ref{eq:gfilters}):


\begin{eqnarray}
	g & = & a\left(\frac{1-\sqrt{3}}{4\sqrt{2}}\right) + b\left(\frac{\sqrt{3}-3}{4\sqrt{2}}\right) + c\left(\frac{3+\sqrt{3}}{4\sqrt{2}}\right) + d\left(\frac{-1-\sqrt{3}}{4\sqrt{2}}\right) \nonumber \\
	 & = & \frac{a\left(1-\sqrt{3}\right) + b\left(\sqrt{3}-3\right) + c\left(3+\sqrt{3}\right) + d\left(-1-\sqrt{3}\right)}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a-a\sqrt{3} + b\sqrt{3} - 3b + 3c+c\sqrt{3} - d - d\sqrt{3}}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a - d + 3\left(c - b\right) + \sqrt{3}\left(b + c - a - d\right)}{4\sqrt{2}} \label{eq:gOptFilter}
\end{eqnarray}

Other ones are described by  (\ref{eq:ihOptFilter}) and (\ref{eq:igOptFilter}), according with their previous definitions in Eqs.(\ref{eq:ihfilters}) and Eq.(\ref{eq:igfilters}), respectively: 

\begin{eqnarray}
	ih & = & a\left(\frac{3-\sqrt{3}}{4\sqrt{2}}\right) + b\left(\frac{3+\sqrt{3}}{4\sqrt{2}}\right) + c\left(\frac{1+\sqrt{3}}{4\sqrt{2}}\right) + d\left(\frac{1-\sqrt{3}}{4\sqrt{2}}\right) \nonumber \\
	 & = & \frac{a\left(3-\sqrt{3}\right) + b\left(3+\sqrt{3}\right) + c\left(1+\sqrt{3}\right) + d\left(1-\sqrt{3}\right)}{4\sqrt{2}} \nonumber \\
	 & = & \frac{3a-a\sqrt{3} + 3b+b\sqrt{3} + c+c\sqrt{3} + d-d\sqrt{3}}{4\sqrt{2}} \nonumber \\
	 & = & \frac{c + d + 3\left(a + b\right) + \sqrt{3}\left(b + c - a - d\right)}{4\sqrt{2}} \label{eq:ihOptFilter}
\end{eqnarray}

\begin{eqnarray}
	ig & = & a\left(\frac{1-\sqrt{3}}{4\sqrt{2}}\right) + b\left(\frac{-1-\sqrt{3}}{4\sqrt{2}}\right) + c\left(\frac{3+\sqrt{3}}{4\sqrt{2}}\right) + d\left(\frac{\sqrt{3}-3}{4\sqrt{2}}\right) \nonumber \\
	 & = & \frac{a\left(1-\sqrt{3}\right) + b\left(-1-\sqrt{3}\right) + c\left(3+\sqrt{3}\right) + d\left(\sqrt{3}-3\right)}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a-a\sqrt{3} - b-b\sqrt{3} + 3c+c\sqrt{3} - 3d+d\sqrt{3}}{4\sqrt{2}} \nonumber \\
	 & = & \frac{a - b + 3\left(c - d\right) + \sqrt{3}\left(c + d - a - b\right)}{4\sqrt{2}} \label{eq:igOptFilter}
\end{eqnarray}

Looking at the original decomposition procedure, shown in Figure~\ref{fig:DaubDandDS}, the complexity of $DecompositionStep$ is $\BigO(3 + (n/2).4 + 3)$, which can be simplified to $\BigO(n)$, and for the main $Decomposition $ algorithm the complexity is $\BigO(1 + \log(n).(n + 1))$, that can be simplified to $\BigO(n\log(n))$.

The complexity of the developed algorithms for decomposition, shown in Figure~\ref{fig:Daub_Opt_D_and_DStep}, are expressed by $\BigO(5 + (n/2).9 + 8)$, or just $\BigO(n)$, for the developed $DecompositionStep$ procedure and $\BigO(1 + \log(n).(n + 1))$, or just $\BigO(n\log(n))$ for the developed $Decomposition$ routine.

Therefore, both original and developed algorithms share the same complexity for normalized filters.

The optimization step to avoid calculations of $\sqrt{2}$ is to perform a non-normalized decomposition by using $4$ instead of $4\sqrt{2}$ as divider in filters $h$ and $g$. 



The next step is to normalize the coefficients after their convolution using the pseudo-code presented in Figure~\ref{fig:Daub_1D_Normalization}.




Since it is executed once on every coefficient of a $n$ sized vector, its complexity is expressed by $\BigO(n)$. Performing a non-normalized transformation and the normalization procedure afterwards, the complexity is $\BigO(n\log(n) + n)$, making the developed optimization slower than the original formulation.

The goal is therefore the gain on accuracy obtained by avoiding a high number of operations using $\sqrt{2}$ and $\sqrt{3}$ from the original $DecompositionStep$ algorithm, as shown in Figure~\ref{fig:DaubDandDS}.

\begin{figure}[h!]
	%\centering
    \includegraphics[width=6.6cm]{Imagens/Daub_Normalization.png}
    \caption{DaWT one dimensional normalization procedure.}
    \label{fig:Daub_1D_Normalization}
 \end{figure}

As presented in Section~\ref{sec:hwt1d_decimated} for the HWT, the optimized composition procedure follows the same basic idea to reduce computation of $\sqrt{2}$, it performs the non-normalized inverse transform by using a denormalization step, shown in Figure~\ref{fig:1DNorm_and_Denorm} before the transformation begins.

In contrast to the HWT optimization strategy for composing the results, the DaWT has to use normalized filters excluding the denormalization step. It is due to the order of coefficients which are calculated by the inverse DaWT, shown in Figure~\ref{fig:Daub_Opt_CStep}, which needs the values to be normalized in order to scale the signals properly. 

\begin{figure}[h!]
	%\centering
    \includegraphics[width=8cm]{Imagens/Daub_Opt_C.png}
    \caption{DaWT one dimensional composition procedure.}
    \label{fig:Daub_Opt_C}
\end{figure}

%\includegraphics[width=10cm]{Imagens/Daub_Opt_CStep.png}

In the 1D DaWT composition procedure and its step procedure, as shown in Figure~\ref{fig:DaubCandCS}, behave similarly to decomposition where its complexity is expressed by $\BigO(n\log(n))$.

The developed optimal $Composition$ procedure, shown in Figure~\ref{fig:Daub_Opt_C}, shares the same complexity of $\BigO(n\log(n))$, but its results are more exact due to the simplified filters, avoiding the unnecessary calculation of $\sqrt{2}$ and $\sqrt{3}$ on every iteration.

\begin{figure}[h!]
	%\centering
    \includegraphics[width=12cm]{Imagens/Daub_Opt_CStep.png}
    \caption{DaWT one dimensional step composition procedure.}
    \label{fig:Daub_Opt_CStep}
\end{figure}

\newpage

\subsection{Two Dimensional} \label{sec:Daub2DOptimization}

The simplifications implemented for the 2D DaWT consider the same principle presented in the 1D case, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{-j/2}$ normalization factor, where $j$ is the corresponding level.

In order to compose or decompose a matrix, the 1D transform is applied in all rows and in all columns of the input matrix in sequence. However, according to \cite{stollnitz1995}, there are two main algorithms for composition and decomposition of matrices, known as Standard and Non-Standard procedures.

During the study of the original DaWT formulation~\cite{DAUBTHESIS}, patterns of normalization factors were recognized. These patterns were analysed and employed in the development of the normalized procedures for the Standard algorithm. In contrast to the 2D HWT optimization, the DaWT Non-Standard procedure does not use the normalization step due to its behavior.

The normalization patterns are illustrated in Figure~\ref{fig:Daub2DNormFactors}, where three decomposition levels are presented for $16$x$16$ matrices. The parameters $ j' $ and $ j'' $ indicate the  normalization levels. The rule to calculate these normalization factors is expressed by $R$ defined in Eq.~(\ref{eq:NormFac}).

\begin{figure}[h!]
  	\centering
  	\includegraphics[width=9cm]{Imagens/DaubNormFactors.png}
  	\caption{DaWT (a) Standard normalization pattern, (b) level combinations.}
	\label{fig:Daub2DNormFactors}
\end{figure}

\begin{figure}[h!]
  	\includegraphics[width=10cm]{Imagens/Daub_Opt_Standard.png}
  	\caption{DaWT 2D Standard normalization step decomposition process.}
	\label{fig:DaubOptStandard}
\end{figure}

The analysis of the original Standard algorithm inherited from the HWT depends on the original normalized decomposition procedure, as shown in Figure~\ref{fig:SDandND}. By executing such algorithm for every row and column of the input matrix, one can observe the corresponding complexity analysis as $\BigO(n\log(n))$. Considering a matrix $C_{N \times N}$, the complexity of the original standard algorithm is $\BigO(n(n\log(n)) + n(n\log(n)))$, or just $\BigO(n^2\log(n))$.

\begin{figure}[h!]
  	\includegraphics[width=10cm]{Imagens/Daub_StandardStepNorm.png}
  	\caption{DaWT 2D Standard normalization step decomposition process.}
	\label{fig:DaubStandardStepNormAlg}
\end{figure}

In opposition to the HWT, the optimal normalized DaWT Standard decomposition has to perform its normalization step, shown in Figure~\ref{fig:DaubStandardStepNormAlg}, in between the decomposition of rows and columns, and a second time at the end of the procedure, as presented in Figure~\ref{fig:DaubOptStandard}.

Since the normalization step procedure is a kernel that will apply the right normalization factor to each coefficient in a $n$x$n$ matrix, its complexity can be expressed as $\BigO(n^2)$. Therefore, the developed optimal Standard normalized decomposition procedure is performed with complexity of $\BigO(n^2\log(n) + n^2)$, or $\BigO(n^2(1 + \log(n)))$. 

Considering this, the developed procedure is slower than the original but it is more accurated due to the new proposed filter set.

The Non-Standard procedure has the same limitation from the Standard where the column operations depend on normalized row operations.

Because of this limitation the Non-Standard algorithm would have to normalize each step level of decomposition before the next operation, instead of adding a normalization step in between each row or column decomposition it is better to perform a normalized procedure using the new set of filters.

This way the Non-Standard algorithm has the same complexity as the same developed procedure from the HWT, as seen in Section~\ref{subsec:cascade2D_Opt_Haar}.

By the same reason that the 1D DaWT does not use any denormalization procedure, the 2D Standard and Non-Standard DaWT does not perform denormalization of its coefficients before composition either.

Both approaches inherit the 2D HWT composition procedures, keeping their original complexity as discussed in Section~\ref{subsec:cascade2D_Opt_Haar}. However, the developed 2D DaWT Composition routines in Int-DWTs library present better results as consequence of using the new set of simplified filters, as discussed in Section~\ref{sec:Daub1DOptimization}.

\section{Interval Compression}
\label{sec:compress}

The main goal of compression procedures is to express an initial data set with the smaller number of points as possible, this task can be done either with or without loss of information~\cite{stollnitz1995},~\cite{kozakevicius2014}.

In the wavelet context, the wavelet expansion of the initial data is analysed in order to decide which wavelet coefficients are more significant than others. Therefore, keeping only the most significant ones, the truncated series represents the compressed data. In~\cite{perin2013} ECG signals were analysed assuming adaptive compression techniques.

In many signal analysis applications, the significant wavelet coefficients are then used  to draw further conclusions about the original data, as for example in~\cite{mohammad2013}, where micro-calcifications in mammograms were detected based on the truncated wavelet representation of the images.

The significance of the coefficients is judged by a threshold value, which can be obtained based on many types of heuristics, linear as the Universal threshold proposed by~\cite{donoho1995} or adaptive as proposed in \cite{bayer2010}. For a review about the many possibilities for choosing threshold values see  \cite{kozakevicius2014}.

By ignoring (and excluding) non significant wavelet coefficients from the wavelet expansion, this strategy turns  the method into a lossy compression procedure. This procedure is called hard thresholding, according to \cite{donoho1995}.

The result is an approximation of the original function.  Figure~\ref{fig:compression}, which can be found in \cite{stollnitz1995}, shows a sequence of approximations, generated by varying the compression rate $s/N$, where $s$ is the number of significant coefficients, and $N$ the total amount of points at the initial data representation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=13.5cm]{Imagens/compression.png}
    \caption{Approximation of functions after compression.}
    \label{fig:compression}
\end{figure}

The hard thresholding is trivial for punctual data, but it can not be applied in the same way to interval information. The punctual algorithm for compression uses a real value $ \tau $ as threshold value, and the decision of the details details which must be ignored, considers a set of simple punctual comparisons.

When treating with interval data, the parameter $ \tau $ becomes an interval, carrying the error in the threshold calculus. This way the compress decision can not be evaluated with the same punctual comparison as before~\cite{Moore79}. Two solutions were developed to manage the decision in this interval extension, the first was named as Hard Decision and the second as Soft Decision, inspired by the nomenclature considered by \cite{donoho1995} for its threshold operators.

\begin{figure}[ht]
    \centering
    \includegraphics[width=13.5cm]{Imagens/C_SDC_HDC.png}
    \caption{Hard Thresholding, Hard Decision and Soft Decision compress procedures.}
    \label{fig:C_SDC_HDC}
\end{figure}

The Hard Decision is made by verifying if the interval data is entirely less than $ \tau $, comparing the left bound of the information and the right bound of the interval parameter. This procedure grants that every possible punctual coefficient is less than all possible punctual values belonging to $ \tau $.

The Soft Decision is made by verifying if most of the interval data is less than the midpoint of $ \tau $, comparing the left bound of the information and the center of the threshold. This procedure grants that most possible punctual coefficients are between the punctual values of $ \overline{\tau} $ and $ \underline{\tau} $.

\chapter{Tests and Results}
\label{sec:tests}

The numerical validation of algorithms proposed and described in Section~\ref{sec:optintform} is based on the application of the HWT for image processing. In this manner, the interval parameters for test executions were obtained from punctual values setting degenerated intervals and using them as input to the interval extensions of both decimated and undecimated implementations of the HWT.

The implementation of interval procedures in the Int-HWT library performs the computation of interval error in the process, presenting the widest interval diameter contained in the transformation results. For all tests, the time measurement carried out by executing each function 30 times, mean and standard deviation values were obtained from those times and used to generate the figures presented in the following subsections.

The tests were executed on an Intel~\textsuperscript{\textregistered} Core~\textsuperscript{\texttrademark} i7 950 Processor @ 3.07GHz, 6GB RAM DDR3 @ 1066MHz, Windows 10 OS, compiled using Microsoft Visual C++ Compiler for Visual Studio 2013 on x64 Release compilation target.

\section{Haar Wavelet Transform}

In order to compute tests using the 1D HWT and its interval extension, a vector filled with 1048576 random values is used as input. For tests using 2D HWT algorithms, the input is generated from a 1024x1024 matrix of random values, configuring 1048576 values.

\subsection{One Dimensional}

The results shown in Figure~\ref{fig:Dec1DHWT} describe the performance of both decomposition and composition procedures from Section~\ref{subsec:1DCascadeOpt}, targeting the decimated and undecimated 1D HWT.

The data indicates that the decimated algorithms developed in this work are $46\%$ faster than the results found in \cite{stollnitz1995}, when composing the results from decomposing the input.

Decomposition, Composition and the combination of both operations, are executed $30\times$, taking $15.46ms$, $16.4317ms$ and $31.1012ms$ with standard deviations of $0.757038ms$, $0.155433ms$ and $0.117835ms$ respectively.

The accuracy gained is from $95\%$ up to $99.8\%$, meaning that the developed algorithms generate much more exact results compared to those from the literature.

Euclidean Distance ($EUC$) and Mean Square Error ($MSE$) are around $99.8\%$ of gain, and Peak-to-Noise Ratio ($PSNR$) is about $24.4\%$.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test1.png}
	\caption{Decimated 1D HWT}
    \label{fig:Dec1DHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test3.png}
	\caption{Undecimated 1D HWT}
	\label{fig:Und1DHWT}
  \end{subfigure}
  \caption{Performance, accuracy and metric gains for the 1D HWT.}
  \label{fig:test1-3}
\end{figure}

The data also indicate that the developed undecimated algorithms are about 30\% slower than the original formulations \cite{IEEE2007} when composing the results from the decomposition step, shown in Figure~\ref{fig:Und1DHWT}.

The average times of execution for decomposition, composition and the combination of both are respectively $43.4929ms$, $12.0031ms$ and $55.0647ms$, and   the corresponding  standard deviation are  $0.385967ms$, $0.440633ms$ and $0.371797ms$ . 

The undecimated formulation adds no error to calculations on the composition step, considering that the input does not contain errors, which explains the lack of bars in Figure~\ref{fig:Und1DHWT}. The accuracy gain when performing the combination of both decomposition and composition is around $81\%$.

The $EUC$ is about $54.5\%$, and $MSE$ is $79.3\%$, while $PSNR$ is $2.15\%$. These results show that despite the loss of performance, the developed algorithms are more accurate than the algorithms from the literature~\cite{IEEE2007}.

\subsection{Two Dimensional}

This Section presents the results of both decimated and undecimated formulations of the HWT in Sections~\ref{sec:results-hwt-2D-dec} and \ref{sec:results-hwt-2D-undec} respectively.

\subsubsection{Decimated}\label{sec:results-hwt-2D-dec}

Data from Figure~\ref{fig:test2S-NS} show the performance of both standard and non-standard approaches for the decimated 2D HWT. The results presented in \mbox{Figure}~\ref{fig:Dec2DSHWT} show a performance boost of $58,1\%$ during the composition step. The average time of execution of the decomposition, composition and the combination of both are $42.2897ms$, $32.5122ms$ and $74.6253ms$, and their standard deviations are $0.271538ms$, $0.487003ms$ and $0.314095ms$ respectively. The accuracy gain of decomposition, composition and the combination of both are $99.8\%$, $93.5\%$ and $98.3\%$, meaning that the results provided from the developed algorithms are much more exact compared to the literature~\cite{stollnitz1995}. The EUC, MSE and PSNR metrics are $99.8\%$, $99.9\%$ and $19.2\%$, showing that the developed algorithms are more accurate than the algorithms from the literature~\cite{stollnitz1995}.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test2S.png}
	\caption{Decimated 2D Standard HWT}
    \label{fig:Dec2DSHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test2NS.png}
	\caption{Decimated 2D Non-Standard HWT}
	\label{fig:Dec2DNSHWT}
  \end{subfigure}
  \caption{Times of execution and error measurement for the 2D HWT using 1024x1024 matrix with random values.}
  \label{fig:test2S-NS}
\end{figure}

Figure~\ref{fig:Dec2DNSHWT} shows the performance, accuracy and metric gains for the decimated 2D non-standard HWT comparing to the original algorithms found in~\cite{stollnitz1995}. The average time of execution for decomposition, composition and both operations are $37.8028ms$, $49.1ms$ and $86.0812ms$, and their standard deviations are respectively $0.985911ms$, $0.362122ms$ and $0.255491ms$. It is important to verify that, as shown in Figure~\ref{fig:2DNormFactors}, the developed non-standard method does not need to perform calculations of $ \sqrt{2} $ due to the implemented algebraic simplifications. Therefore, the corresponding calculations do not increase the error during this process, indicated by accuracy values at $100\%$ for all three methods. Due to the lack of error, the EUC and MSE are also shown at $100\%$. The PSNR metric cannot be calculated due to division by $0$, since it uses MSE as divisor and its value is $0$.

\subsubsection{Undecimated}\label{sec:results-hwt-2D-undec}

Figure~\ref{fig:test4S-NS} shows the performance impact of both standard and non-standard approaches for the undecimated 2D HWT.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test4S.png}
	\caption{Undecimated 2D Standard HWT}
    \label{fig:Und2DSHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test4NS.png}
	\caption{Undecimated 2D Non-Standard HWT}
	\label{fig:Und2DNSHWT}
  \end{subfigure}
  \caption{Times of execution and error measurement for the 2D HWT using 1024x1024 matrix with random values on 4 levels of decomposition.}
  \label{fig:test4S-NS}
\end{figure}

The performance shown in Figure~\ref{fig:Und2DSHWT} and Figure~\ref{fig:Und2DNSHWT} indicates that the developed algorithms are at least $10\%$ slower than the originals, despite the composition step which has a little advantage of $5\%$ on the standard method.

For the standard approach, the average execution time for decomposition, composition and the combination of both are $195.461ms$, $290.717ms$ and $200.601ms$, and their standard deviations $3.43775ms$, $10.5094ms$, $3.13332ms$.

For the non-standard approach execution time averages  are $173.385ms$, $286.755ms$ and $192.677ms$, and their standard deviations are $3.03845ms$, $4.40987ms$ and $2.65613ms$.

Both approaches also share the same percentage of accuracy gain, $92.6\%$ and $87.4\%$ respectively for decomposition and the combination of both algorithms performing together. The $EUC$, $MSE$ and $PSNR$ of both approaches are around $51\%$, $76\%$ and $2\%$ respectively.

\section{Daubechies Wavelet Transform}

In order to evaluate the 1D DaWT and its interval extension, three tests are executed using vectors filled with $1048576$, $4194304$ and $16777216$ random values as input respectively. For tests using 2D DaWT algorithms, the inputs were generated from $1024$x$1024$, $2048$x$2048$ and $4096$x$4096$ matrices of random values.

\subsection{One Dimensinal}

The results shown in Figure~\ref{fig:test5} describe the performance of both decomposition and composition procedures from Section~\ref{sec:1DDaWT}, targeting the one dimensional DaWT. 

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-1048576.png}
	\caption{Using 1048576 values}
    \label{fig:1DDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-4194304.png}
	\caption{Using 4194304 values}
	\label{fig:1DDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-16777216.png}
	\caption{Using 16777216 values}
	\label{fig:1DDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 1D DaWT.}
  \label{fig:test5}
\end{figure}

The data indicate that the developed algorithms are $35\%$, $26\%$ and $26\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing the vectors, and its coefficient of variation (CV) are $13.6\%$, $22.4\%$ and $17.6\%$ respectively. The average times of executions of the optimal algorithms for decomposition are $16.0975ms$, $69.0551ms$ and $283.37ms$, and the corresponding standard deviation are  $0.219135ms$, $1.55045ms$ and $5.00761ms$.

The Composition tests indicates that the developed algorithms are $84\%$, $78\%$ and $68\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $5.7\%$, $5.3\%$ and $9.8\%$. The average times of executions of the optimal algorithms for composition are $21.2527ms$, $88.8472ms$ and $363.605ms$, and the corresponding standard deviation are  $0.121824ms$, $0.474684ms$ and $3.57715ms$.

The combination of both Decomposition and Composition operations are $64\%$, $53\%$ and $49\%$ and its CV are $7.5\%$, $9.4\%$ and $9.7\%$. The average times of executions are $37.2334ms$, $157.169ms$ and $646.311ms$, and the corresponding standard deviation are  $0.281221ms$, $1.47963ms$ and $6.30055ms$.

The interval extension shows that the results are $343$x, $22$x and $203629$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT, its due to the proposed filter presented in Section~\ref{sec:1DDaWT}.

The results from $EUC$ are $68.3\%$, $68.3\%$ and $68.3\%$, data from $MSE$ indicate gains of $89.9\%$, $91.4\%$ and $91.9\%$, and $PSNR$ gains are $13.7\%$, $18.1\%$ and $22.4\%$. The results from $EUC$, $MSE$ and $PSNR$ show a very good gain over the original formulations of the DaWT.

\subsection{Two Dimensinal}

This Section presents the results of both Standard and Non-Standard approaches of the DaWT in Sections~\ref{sec:DaWT-results-Standard} and \ref{sec:DaWT-results-Non-Standard} respectively.

\subsubsection{Standard}\label{sec:DaWT-results-Standard}

The results shown in Figure~\ref{fig:test6S} describe the performance of both decomposition and composition procedures from Section~\ref{sec:2DDaWT}, targeting the two dimensional Standard DaWT.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-1024.png}
	\caption{Using 1024x1024 matrix}
    \label{fig:2DSDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-2048.png}
	\caption{Using 2048x2048 matrix}
	\label{fig:2DSDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-4096.png}
	\caption{Using 4096x4096 matrix}
	\label{fig:2DSDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 2D Standard DaWT.}
  \label{fig:test6S}
\end{figure}

The data indicate that the developed algorithms are $25\%$, $23\%$ and $17\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing matrices, and its coefficient of variation (CV) are $7.5\%$, $10.5\%$ and $2.2\%$ respectively. The average times of executions are $43.1052ms$, $195.124ms$ and $1052.4ms$, and the corresponding standard deviation are  $0.323927ms$, $2.03999ms$ and $2.34561ms$.

The Composition tests indicates that the developed algorithms are $63\%$, $56\%$ and $39\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $4\%$, $9.6\%$ and $1.6\%$. The average times of executions are $55.1799ms$, $241.814ms$ and $1238.55ms$, and the corresponding standard deviation are  $0.220524ms$, $2.32843ms$ and $2.01274ms$.

The combination of both Decomposition and Composition operations are $44\%$, $40\%$ and $28\%$ and its CV are $3.3\%$, $7.1\%$ and $3.3\%$. The average times of executions are $98.1822ms$, $436.064ms$ and $2295ms$, and the corresponding standard deviation are  $0.325826ms$, $3.11464ms$ and $7.64587ms$.

The interval extension shows that the results are $220$x, $18$x and $80236$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT. The results from EUC are $66.4\%$, $70\%$ and $70\%$, data from MSE indicate gains of $88.7\%$, $90.8\%$ and $91.2\%$, and PSNR gains are $3.2\%$, $3.6\%$ and $3.6\%$. The results from EUC, MSE and PSNR show a very good gain over the original formulations of the DaWT.

\newpage

\subsubsection{Non Standard}\label{sec:DaWT-results-Non-Standard}

The results shown in Figure~\ref{fig:test6NS} summarize the performance of both decomposition and composition procedures from Section~\ref{sec:2DDaWT}, targeting the two dimensional Standard DaWT.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-1024.png}
	\caption{Using 1024x1024 matrix}
    \label{fig:2DNSDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-2048.png}
	\caption{Using 2048x2048 matrix}
	\label{fig:2DNSDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-4096.png}
	\caption{Using 4096x4096 matrix}
	\label{fig:2DNSDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 2D Non Standard DaWT.}
  \label{fig:test6NS}
\end{figure}

The data indicate that the developed algorithms are $40\%$, $34\%$ and $21\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing the matrices, and its coefficient of variation (CV) are $6.3\%$, $7.1\%$ and $3\%$ respectively. The average times of executions are $48.6368ms$, $221.135ms$ and $1277.12ms$, and the corresponding standard deviation are  $0.306729ms$, $1.57093ms$ and $3.8811ms$.

The Composition tests indicate that the developed algorithms are $29\%$, $25\%$ and $15\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $4.7\%$, $15.6\%$ and $2.3\%$. The average times of executions are $65.2928ms$, $302.765ms$ and $1791.56ms$, and the corresponding standard deviation are  $0.310508ms$, $4.74117ms$ and $4.07062ms$.

The combination of both Decomposition and Composition operations are $34\%$, $28\%$ and $18\%$ and their CV are $4\%$, $6.7\%$ and $2.7\%$. The average times of executions are $113.875ms$, $522.279ms$ and $3067.84ms$, and the corresponding standard deviation are  $0.4643ms$, $3.53883ms$ and $8.20209ms$.

The interval extension shows that the results are $3034$x, $67$x and $5916370$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT.

The results from $EUC$ are $47\%$, $43.3\%$ and $45\%$, data from $MSE$ indicate gains of $71.9\%$, $67.6\%$ and $70\%$, and $PSNR$ gains are $1.8\%$, $1.6\%$ and $1.7\%$. The results from $EUC$, $MSE$ and $PSNR$ show a very good gain over the original formulations of the DaWT.

\chapter{Conclusion}
\label{sec:conclusion}

This work presented an interval implementation of the Haar Wavelet Transform (HWT), previously published in~\cite{7}, for both decimated~\cite{weit2013} and undecimated~\cite{sim2015} approaches and covering all study cases. The same study and optimization strategies were applied on the Daubechies Wavelet Transform (DaWT) with success.

The optimization for the decimated HWT made possible a gain of both accuracy of results and accuracy on performing the calculations, as it was expected. The same optimization was applied to the undecimated HWT, also know as À-Trous approach, and good results were obtained from the accuracy analysis, but there were performance loss due to the addition of operations on top of the original formation from the literature~\cite{stollnitz1995}.

A new set of filters was proposed by simplifying the DaWT original filters~\cite{DAUBTHESIS} in order to increase accuracy of results. The algebraic manipulation of its filters made possible a gain in terms of accuracy of results in both 1D and 2D procedures. However, there was a need to add normalization steps on the top of the original DaWT formulation which resulted in a loss of performance.

Good results related to error analysis were obtained, that is, the optimizations improve accuracy. The metrics used in this work helped to evaluate the accuracy of all presented implementations.

As the wavelet transforms are appropriated to data analysis in contexts that the scales of representation are relevant to the problem, the accuracy gain obtained with the proposed simplifications in this work represent a significant contribution to this research area.

Further research considers the study of other Discrete Wavelet Transforms (DWTs), together with the corresponding parallel and/or distribution of the Int-DWTs Library, using massive parallel architectural of GPUs and considering CUDA programming language for fast computation.


\bibliography{bibliografia}
\bibliographystyle{abnt}



\end{document}

\grid
