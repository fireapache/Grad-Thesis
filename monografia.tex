\documentclass[tcc,capa]{texufpel}

\usepackage[latin1]{inputenc} % acentuacao
\usepackage{graphicx} % para inserir figuras
\usepackage{times,amsmath,amsfonts,xspace,dsfont,pgf,latexsym,amssymb}
\usepackage{todonotes}
\usepackage{caption}
\captionsetup{compatibility=false}
\usepackage{subcaption}
\usepackage[linesnumbered]{algorithm2e}

\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line

\newcommand{\red}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\blue}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\DeclareMathSymbol{\Theta}{\mathalpha}{operators}{2}
\newcommand{\BigO}{\ensuremath{\operatorname{O}}}

\unidade{Centro de Desenvolvimento Tecnológico}
\curso{Ciência da Computação}
\nomecurso{Bacharelado em Ciência da Computação}
\titulocurso{Bacharel em Ciência da Computação}
\title{Int-DWTs Library - Algebraic Simplifications \\ Increasing Performance and Exactitude \\ of Discrete Wavelet Transforms}

\author{dos Santos}{Vinícius Rodrigues}
\advisor[Profa.~Dra.]{Reiser}{Renata Hax Sander}
\coadvisor[Prof.~Dr.]{Pilla}{Maurício}
\collaborator[Profa.~Dra.]{Kozakevicius}{Alice}

\keyword{wavelet}
\keyword{interval}
\keyword{performance}
\keyword{optimization}

\begin{document}

%\renewcommand{\advisorname}{Orientadora}           %descomente caso tenhas orientadora
%\renewcommand{\coadvisorname}{Coorientadora}      %descomente caso tenhas coorientadora

\maketitle 

\sloppy



\begin{englishabstract}%
  {Algebraic Simplifications Increasing Performance And Exactitude of Discrete Wavelet Transforms}%
  {wavelet, interval, performance, optimization, haar, daubechies, accuracy, exatitude}
  
This project describes the main results consolidating the Interval Methods of the Discrete Wavelet Transforms Library (Int-DWTs), providing algebraic simplifications in order to increase performance and guarantee exactitude of Discrete Wavelet Transforms (DWTs). The methods in the Int-DWTs include interval extensions and corresponding optimizations to the Haar Wavelet Transform (HWT) and Daubechies Wavelet Transform (DWT), which are implemented by using C-XSC. Both steps in the HWT, meaning as Cascade and à trous algorithms, are extended to the interval approach. Optimizations for the normalized formulations of the one- and two-dimensional transforms to increase speed and guarantee accuracy of results are also presented. As an application, image compression is addressed, whose quality is computed and compared by different metrics. The error analysis of the different interval formulations as well as the speedup obtained through the interval formulations and the proposed simplifications are analyzed. The results consider a study over the interval extension of DWT, deducting simplifications and implementing optimizations based on the analogous methodology performed over HWT approaches. In addition, not only interval algorithms are developed but also metrics for evaluating procedure's accuracy are considered. By assuming concepts from Interval Mathematics, it enables us to perform interval analysis and efficiently manage of computing errors of DWTs. The metrics being used to measure result quality in the Int-DWTs are the following: Euclidean Distance, Mean Squared Error and Peak signal-to-noise ratio. In contexts that the scales of representation are relevant to the problem, the accuracy gain obtained with the proposed optimizations in the Int-DWTs represent a significant contribution to the scientific computing research area. 

\end{englishabstract}

%Lista de Figuras
\listoffigures

%Lista de Tabelas
%\listoftables

%lista de abreviaturas e siglas
\begin{listofabbrv}{SPMD}
        \item[DWTs] Discrete Wavelet Transforms
        \item[DaWT] Daubechies Wavelet Transform
        \item[HWT] Haar Wavelet Transform
        \item[EUC] Euclidean Distance
        \item[MSE] Mean Squared Error
        \item[PSNR] Peak Signal to Noise Ratio
\end{listofabbrv}

%Sumario
\tableofcontents

\chapter{Introduction}

The quality of numerical results in Scientific Computing (SC) depends on understanding the different error causes and on controling their propagation,  as well as improving  computations upon the involved procedures.
%as well as improving the corresponding computations  \red{concerned with} the involved %procedures.
This study considers the Interval Mathematics (IM) approach and proposes a solution based on Moore's arithmetic~\cite{Moore79} for the implementation of the Haar wavelet transform (HWT), considering two well established algorithms, Cascade and {\it à trous}, and their standard formulations assuming point values as input data, being the first step on the development of the Int-DWT library that will provide interval results for several Discrete Wavelet Transforms (DWTs).

Interval results carry over the safety of their quality together with the degree of their uncertainty~\cite{Moore79}. The diameter of the interval solution represents with fidelity the uncertainties of input parameters, being also an indicative of the error influence and of the extension of its propagation within the incoming data. Interval solutions also indicate truncation and rounding errors contained in the computed results.

In the last decade, many studies associating wavelet transforms with interval mathematics have arisen, promoting a new research direction and pointing out the relevance of  interval computations for a wide range of applications. Shu-Li et al.~\cite{shu-li2005} propose the interval extension of the interpolating wavelet family for the construction of an adaptive algorithm for solving partial differential equations (PDE). Another collocation method for solving PDE is presented by Liu~\cite{liu2013}, based on the interval extension for the Shannon-Garbor wavelet family. With respect to image processing applications, Minamoto and Aoki~\cite{minamoto2010} propose a blind digital image watermarking method using interval Daubechies wavelets.

The main motivation for interval techniques integrated with  DWT is to provide trustworthy and validated results to technological and SC applications assuming such transformations. The pool of techniques involving wavelets is ample, specially in signal and image processing~\cite{5,6}.
The  HWT, the  simplest discrete wavelet transform~(DWT) in terms of algorithm complexity, is still nowadays largely explored,  being its robustness with respect to many different mathematical structures and space formulations  a relevant aspect~\cite{novikov2012} addressed in many applications.
%\todo[inline]{Pilla: Só porque ela é a mais simples não deveria ser explorada? Ficou meio repetitivo também.} Oi, Pilla, eu nao achei repetitivo...mas fica ah vontade para mudar..eu li li e acabo gostando da frase...
Besides, as demonstrated in \cite{brito1998}, the imaging inaccuracy of the Haar wavelet basis is the smallest possible, motivating its usage in many 2D problems as well as the current formulation in the interval context.

%Thus, the present study introduces an integrated analysis of both, the original %HWT~\cite{stollnitz1995} and the development of its interval extension,  called Int-HWT,  %which deals with \red{imprecise input data} and calculation erros. Here both 1D and 2D cases %are covered. The present work  also addresses  the analysis of a threshold procedure, %considered as a significant part of many compression and filtering algorithms for treating %signals and images.
%this present 
Hence, the present study introduces an integrated analysis of the original HWT algorithms and the proposed interval extensions.  Preliminary results of the new interval extension
of the Int-HWT library presented in~\cite{7} are revisited here, including the interval extensions of the decimated HWT~\cite{stollnitz1995} (Cascade algorithm) for the 1D and 2D cases. The current research addresses as well the {\it À-Trous} algorithm, a non-decimated HWT formulation ~\cite{IEEE2007,kozakevicius2013}, proposing interval extensions for both  1D and 2D cases. Furthermore, a threshold procedure for treating signals and images, considered as a significant part of many compression and filtering algorithms, is analyzed and its interval extensions are presented, namely the Hard and Soft Decision  precudures. 

During the study of the original normalized and non-normalized forms of the HWT \cite{stollnitz1995}, the possibility to optimize them arises from the observation of the patterns of the transform filters in each decomposition level. Therefore, algebraic simplifications are executed to eliminate operations with irrational numbers, originally responsible for the normalization of the transform and considered in each iteration of the original algorithms \cite{weit2013}. Through these simplifications, a gain of precision in the interval formulations is obtained for the normalized decimated 1D HWT, producing therefore more reliable results. In the current work, a similar heuristic is considered to simplify the standard formulation of the non-decimated HWT, generating interval extensions implemented again in an optimal way.

The C-XSC library~\cite{holbig05} is employed to support the implementation of all formulations addressed by the Int-HWT library. Such open source  library consists of a set of tools for the development of numerical algorithms, integrating precision and automatic verification of the results, frequently applied in SC~\cite{cxsclink}. The compatibility between C++ and C-XSC library allows high level programming techniques for numerical applications~\cite{holbig05a}, aggregating other available libraries to support SC and also including additional packages in the source code~\cite{cxsc3}.

%To make interval results able to contain all possible punctual results, algorithms performing only interval arithmetic have to be designed. In this context, the HWT and its many formulations are extended according to the interval arithmetic, considering the C-XSC interval library~\cite{cxsclink}.

This paper is organized as follows :
Section~\ref{sec:intarit} summarizes some aspects about the interval computation, supporting the extensions proposed in the remaining of the article.
%Sections~\ref{sec:hwt-decimated} and \ref{sec:hwt-undecimated} present the decimated and undecimated formulation of the HWT. 
The C-XSC library and 
the proposed approach for interval data compression
are presented in section~\ref{sec:optintform}.
Afterwards, in Section~\ref{sec:optintform} the developed optimizations as well as the thresholding procedures are also presented. Interval metrics are introduced in Section~\ref{sec:metrics} and Section~\ref{sec:tests} presents tests and results. Finally, conclusions and the future works are shown in Section~\ref{sec:conclusion}.

\chapter{Interval Mathematics}\label{sec:intarit}

\section{Concept of Interval Aritmethics}

Interval analysis was developed by Ramon Moore in the 1960's~\cite{Moore59a,MoorePhD}, when computers were still at an early stage of development and every additional cost associated with keeping track of the computational errors were considered as too high. Furthermore, the produced error bounds were overly pessimistic and therefore quite useless.

Nowadays, the research for new interval methods have reached a high scientific level, producing tight error bounds faster than approximations of non-rigorous computations. Even in pure mathematics, non-trivial results have recently been proved using computer-aided methods based on interval techniques~\cite{Tucker11}. Additionally, scientific computations demanding rigor as well as speed from numerical computations should be performed based on  techniques for performing validated numerical calculations provided by interval methods.

Distinct approaches of arithmetic operations have been discussed in the literature, see for example \cite{Walster97a,Hickey2001a} and~\cite{FS04}.

Despite distinct approaches of arithmetic for  intervals, see e.g.~\cite{Walster97a,Hickey2001a} and~\cite{FS04}, this paper considers the interval analysis proposed by Moore in the 1960's, providing methods to obtain accuracy in numerical calculations~\cite{Moore79,Moore59a,MoorePhD}.
%
%\subsection{Moore Interval Arithmetic Operations}\label{subsec:Moore}
%
%\red{Temos mesmo que explicar tudo isso? Não podemos citar o trabalho quando necessário? Pilla}
%
In Moore arithmetic, an interval $X$ is a continuum subset of real numbers which is in the infimum-supremum representation given as:
\begin{equation}
   X=\{x\in \mathbb{R} \colon a \leq x \leq b, a,b \in \mathbb{R}, a \leq b\}.
\end{equation}
%and in the midpoint-radius representation, expressed by:
%\begin{equation}
%   X=\{x\in \mathbb{R} \colon | x- a | \leq \alpha, a \in \mathbb{R},  0 \leq \alpha  \in \mathbb{R}\}.
%   \end{equation}
%
%The two representations are identical for real intervals but not for floating-point intervals.  The infimum-supremum arithmetic is used in computers, since arithmetic based on midpoint-radius representation demands overestimation for multiplication and division. In general, it is not exactly representable in floating point causing additional computational effort.
The set of all real intervals is indicated by $\mathbb{I}\mathbb{R}$. Frequently, an interval $X\in \mathbb{I}\mathbb{R}$ is indicated by its endpoints, $X=[a,b]$.  When $a=b$ then $[a,b] \in \mathbb{I}\mathbb{R}$ is called a degenerate interval. For an unary operator $\omega:\mathbb{I}\mathbb{R} \rightarrow \mathbb{I}\mathbb{R}$, we have that $\omega(X)= \{\omega(x) : x \in X\}$.
For an arithmetic operation $\ast: \mathbb{I}\mathbb{R}^2 \rightarrow \mathbb{I}\mathbb{R}$ such that $\ast \in \{ +,-, /, \cdot \}$,
the following property is valid for any two intervals $X$ and $Y$:
$X\ast Y =\{x \ast y \colon x \in X \mbox { and } y \in Y\}$.

Interval functions can be computed by performing arithmetic operations or by applying rational approximation methods~\cite{Walster97a}. The former identifies the class of rational interval functions and the latter, the class of irrational interval functions. Thus, Moore arithmetic guarantees correctness in the sense that any computation performed with standard floating-point methods can also be done with their interval version. By considering the set $\Re = [-\infty, \infty]$ of extended real numbers,  the related family of subintervals of $[a,b]$ is given as
% \todo[inline]{OK.
%Pela propriedade de fechamento de algumas operacoes sobre intervalos. Pessoal. O mesmo para %intervalos de extremos de numeros de maquinas.
% Desculpa,  Uma duvida: Porque precisamos definir um conjunto de extended real numbers ? este %conjunto nao eh o conjunto dos reais?$\mathbb{R}$?}
\begin{equation}\label{eq-3}
\mathbb{I}_{[a,b]} = \{[x,y]  \subseteq \Re \colon  a\leq x\leq y\leq b \}.
\end{equation}
In particular, let $U$ be the real unit interval $U=[0,1]\subseteq \Re$.
By Eq.~(\ref{eq-3}), the set of all subintervals of $U$ is indicated as
$\mathbb{I}_U=\{[x,y]\subseteq \Re \colon 0\leq x\leq y\leq 1\}$.
The  projections $l,r:\mathbb{I}_{[a,b]}\rightarrow [a,b]$  are respectively given by
$l([x,y])=x$ and  $r([x,y])=y$, for all  $[x,y] \in \mathbb{I}_{[a,b]}$.
Additionally, when $X=[x,y] \in \mathbb{I}_{[a,b]}$, the projection functions  $l(X)$ and $r(X)$ are also denoted by $\underline{X}$ and $\overline{X}$,
respectively. Thus, for all $X,Y \in \mathbb{I}_{[a,b]}$, interval arithmetic operations can be defined as follows:
\begin{eqnarray*}
    X\pm Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm} [\underline{X}\pm \underline{Y},\overline{X}\pm \overline{Y}]; \hspace{3.0cm} 
%     X-Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm}[\underline{X}-\overline{Y},\overline{X}-\underline{Y}] ;\\
      1/Y \hspace{-0.1cm}=\hspace{-0.1cm} [1/\overline{Y},1/\underline{Y}] , \mbox { \,\,\,\, if \,\, $0 \notin Y$};\\
      X\cdot Y \hspace{-0.1cm}&\hspace{-0.1cm}=\hspace{-0.1cm}&\hspace{-0.1cm} [\min\ \{\underline{X}\cdot\underline{Y},\underline{X}\cdot\overline{Y},
\overline{X}\cdot\underline{Y},\overline{X}\cdot\overline{Y}\},\max
\{\underline{X}\cdot\underline{Y},\underline{X}\cdot\overline{Y},
\overline{X}\cdot\underline{Y},\overline{X}\cdot\overline{Y}\}].
\end{eqnarray*}
%
%Other operators considered in this study are described below:
%Let $X\in \mathbb{I}_{[a,b]} $ and $n\in \mathbb{N}$. 
Additionally, the power and root operations are, respectively,  defined as follows:
\begin{align*}
 X^n &= \left \{
\begin{array}{ll}
[\underline{X}^n, \overline{X}^n], & \mbox{ if } \underline{X} > 0 \mbox{ or } n \mbox{  is odd;} \\ \relax
%
[\overline{X}^n, \underline{X}^n], & \mbox{ if } \overline{X} < 0 \mbox{ and } n \mbox{  is even;} \\ \relax
%
[0, \left( \max\{\underline{X}, \overline{X}\} \right)^n], & \mbox{ if } \ 0 \in X \mbox{ and }   n \mbox{  is even}.
\end{array}
\right. \\
%
\sqrt[n]{X} &= \left \{
\begin{array}{ll}
[\sqrt[n]{\underline{X}}, \sqrt[n]{\overline{X}}], & \mbox{ if } \underline{X} > 0 \mbox{ or } n \mbox{  is odd;} \\ \relax
%
\mbox{indefined, \hspace{1.7cm}} & \mbox{otherwise.} \relax
\end{array}
\right.
\end{align*}

The partial order used here in the context of interval mathematics is the Product (or Kulisch-Miranker) order and, for all $X,Y \in \mathbb{I}_{[a,b]}$, it is defined as follows:
\begin{equation}
X\leq_{\mathbb{I}_{[a,b]}} Y \mbox{ iff } \underline{X}\leq
\underline{Y} \mbox{ and } \overline{X}\leq \overline{Y}.
\end{equation}
Additionally, an interval function preserving the partial order $\leq_{\mathbb{I}_{[a,b]}}$ is called $\mathbb{I}_{[a,b]}$-monotonic function with respect to the partially ordered set $(\mathbb{I}_{[a,b]}, \leq_{\mathbb{I}_{[a,b]}})$.

%%%%%%%%%%%%%%
% \subsection{Interval Extensions and Representations}
% %%%%%%%%%%%%%%
% It is possible to extend all standard (exponential, logarithm, trigonometric) real function $f$  to an interval function $F$. As pointed out in~\cite{Tucker11}, elementary functions, which are obtained by combining  constants, arithmetic operations and  compositions of standard functions, do not provide, in general, well-defined interval extensions. This can be observed for a set of singular points of a real function $f$, which frequently can be  included in the corresponding  natural extension. However,  the interval evaluation $F(X)$ produces the exact range of $f$ over the domain $X \in\mathbb{I}_{[a,b]}$.

% By the Fundamental Theorem of Interval Analysis\cite{Tucker11},
% let $f$ be an elementary function  and $F$ be an interval extension  such that $F(X)$ is well-defined, for $X \in\mathbb{I}_{[a,b]}$. Then, $F$ satisfies the following properties:
% \begin{description}
% \item [(i)] \textbf{Inclusion Monotonicity}: $Z \subseteq Z' \subseteq X \rightarrow F(Z) \subseteq F(Z') \subseteq F(X)$ ;
% \item [(ii)] \textbf{Range Enclosure}: $f[X] = f(X)$.
% \end{description}


% An interval $X \in\mathbb{I}_{[a,b]}$ is said to be an interval representation of a real number $\alpha$, if $\alpha \in X$.
% %Considering two interval representations $X$ and $Y$ of a real number $
% %\alpha$,  $X$ is said to be a better representation of
% %$\alpha$ than $Y$ if $X$ is narrower than $Y$, that is,  if $X\subseteq Y%$.
% This notion can be easily extended for
% tuples of $n$ intervals $(\vec{X}) = (X_1, \ldots, X_n)$.
% In~\cite{Moore59a}, a function $F:\mathbb{I}_{[a,b]}^n \rightarrow  \mathbb{I}_{[a,b]}$ is an \textbf{interval
% representation} of a function $f:[a,b]^n \rightarrow  [a,b]$ if, for each
% $\vec{X}\in {\mathbb{I}_{[a,b]}}^n$ and $\vec{x}\in \vec{X}$, $f(\vec{x})\in
% F(\vec{X})$.
% From the previous paragraph, an interval function may be seen as a representation of a subset of real numbers $X$ defined as the range $f(X)$.
% %Now, extending the previous discussion, an interval function $F:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$ is a better
% %interval representation of the function $f:U^n \rightarrow  U$ than $G:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$, denoted by $G\sqsubseteq
% %F$, if for each $\vec{X}\in \mathbb{I}_{[a,b]}^n$, the inclusion $F(\vec{X})\subseteq G(\vec{X})$ holds.
% Considering  $F:\mathbb{I}_{[a,b]}^n \rightarrow \mathbb{I}_{[a,b]}$, let  $\underline{F},\overline{F}:U^n \rightarrow U$ be the functions
% respectively given by
% \begin{eqnarray}\label{eq-projection-Fu}
% \underline{F}(x_1,\ldots,x_n) & = &
% l(F([x_1,x_1],\ldots,[x_n,x_n])) \\ \overline{F}(x_1,\ldots,x_n) &
% = &
% r(F([x_1,x_1],\ldots,[x_n,x_n])).\label{eq-projection-Fo}\end{eqnarray}

% The fundamental theorem of interval arithmetic implicitly
% rests on the notion of correctness. It means that, when $F$ is an inclusion monotonic interval extension of a real function $f$,
% then $f(X_1,\ldots,X_n)\subseteq
% F(X_1,\ldots,X_n)$. Thus,  $F(X)$
% contains the range $f(X)$ and consequently,  the class of
% inclusion monotonic interval functions have the property of
% correctness. Moreover, since all arithmetic operations are inclusion
% monotonic functions, it holds that any rational interval function is also correct.
% In this paper, we consider the interval extension of three  real functions, which are studied in the following.

%%%%%%%%%%%%
\section{Interval Metrics}\label{sec:metrics}
%%%%%%%%%%%%%%%%%%
%Not only interval algorithms managing computation error  but also metrics for evaluation and %comparison of image evolved in the DWTs use concepts from Interval Mathematics \cite{Moore79} %to perform interval analysis.   , evolved in the DWTs

Not only interval algorithms  but also metrics for evaluating procedure's accuracy can assume concepts from Interval Mathematics \cite{Moore79} to perform interval analysis efficiently managing computation errors.
The motivation to consider numerical intervals instead of simple punctual values is linked to the capability of intervals to represent infinite punctual values. This sort of representation is very useful in SC when the accuracy of the input (or output) data is not granted. In these cases of uncertainty or inaccuracy the interval procedures should ensure that all possible punctual results belong to the interval results.
In addition, due to memory limitation, it is also common to compute round (or simply truncated) values to store the result afterwards. This heuristic may result in different values, depending on the machine's configuration where the program has been executed. The metrics being used to measure result quality are presented below.

\subsection{Euclidean Distance}
Let $\tilde{Y} = (\tilde{y})_{ij} \in \mathbb{R}^{n\times m}$ be an estimator of $Y = (y)_{ij} \in \mathbb{R}^{n\times m}$ whose  $nm$ elements  $\tilde{y}_{ij}$ are predictions of the original values $(y)_{ij}$.
The Euclidean distance between  $Y$  and its estimator $\tilde{Y}$ is defined by the following expression:
\begin{equation}\label{eq-5}
D(\tilde{Y},Y) =  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{y}_{ij} - y_{ij})^2}.
\end{equation}

Analogously, $\tilde{\mathbb{Y}}= (\tilde{\mathbf{Y}})_{ij}$ is called an interval estimator of $\mathbb{Y}= (\mathbf{Y})_{ij}\in \mathbb{IR}^{n\times m}$ with an $nm$-dimensional matrix of interval predictions $\tilde{\mathbf{Y}}_{ij}$ of the original interval quantities $\mathbf{Y}_{ij}$. An interval extensional of Eq.(\ref{eq-5-1}) is given in the following:

\begin{equation}
\label{eq-5-1}
\mathbf{D}(\tilde{\mathbb{Y}}, \mathbb{Y}) =  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n} \left( \tilde{\mathbf{Y}}_{ij} - \mathbf{Y}_{ij} \right)^2}.
\end{equation}

\subsection{Mean Squared Error}
 The  Mean Squared Error (MSE) is considered as a risk function, corresponding to the expected value of the squared error loss.
It is an estimator measuring the average of the squares of the errors in SC, providing the difference between the estimator and what is estimated. MSE allows us to compare the pixel values of our original image to our degraded (noise) image based on the amount by which the values of the original image differ from the degraded image.

Let $Y=(y)_{ij} \in \mathbb{R}^{n\times m}$ be the related matrix of true values $y_{ij}$.  The accuracy of $\tilde{Y} = (\tilde{y})_{ij}$ can be
obtained by the application of an MSE operator as the following
\begin{equation}\label{eq-6}
MSE ( \tilde{Y},Y) = \frac{1}{mn} \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{y}_{ij} - y_{ij})^2} = \frac{1}{mn} D(\tilde{Y},Y).
\end{equation}

Analogously, for all $\mathbb{Y}= (\mathbf{Y})_{ij}\in \mathbb{IR}^{n\times m}$  corresponding to the related matrix of true values $\mathbf{Y}_{ij}$,   the accuracy of the estimated values can be obtained by applying  an interval extension of the $MSE$ operator in  Eq.~(\ref{eq-5}), which is given by the following expression:
\begin{equation}
\mathbf{MSE} (\tilde{\mathbf{Y}},\mathbf{Y}) = \frac{1}{mn}  \sqrt{\sum_{j=0}^{m}\sum_{i=0}^{n}(\tilde{\mathbf{Y}}_{ij} - \mathbf{Y}_{ij})^2}= \frac{1}{mn} \mathbf{D}(\tilde{\mathbb{Y}}, \mathbb{Y}) .
\end{equation}

%\red{Alice 6/10: Duvida aqui. as medidas MSE nao deveriam medir a qualidade dos valores estimados em relacao ao valores reais? Neste sentido, nao deveriamos escrever $\mathbf{MSE}(\tilde{\mathbf{Y}})  $???}

% Vinícius { Alice, ? é a imagem aproximada e Y é a original, está correto dessa maneira!} -> aLICE:OK!!!

\subsection{Peak signal-to-noise ratio}
%
%An important performance metric for evaluation and comparison of image or video codecs is 
The Rate/Distortion (R/D) measures the image quality in terms of Peak Signal-to-Noise Ratio (${PSNR}$), expressing the ratio between the maximum possible power of a signal and the power of corrupting noise affecting the fidelity of its representation.  $PSNR$ is usually given by the logarithmic decibel scale and  most commonly used to measure the quality of reconstruction of lossy compression codecs. In image compression, the signal is the original data and the noise is the error introduced by compression. 
%However, the range of validity of this metric is limited since it is only conclusively valid when used to compare results from the same content. 
Thus, the expression of ${PSNR}$ is most easily defined via the logarithmic decibel scale related to the $MSE$ as follows:
\begin{equation}\label{eq-8a}
PSNR (\tilde{Y},Y) = 10 \cdot \log_{10} \frac{MAX_I^2}{MSE(\tilde{Y},Y)} ,
\end{equation}
when $I$ indicates a $mn$-dimensional monochrome image associated to $Y$ and  $MAX_I$  is  the maximum possible pixel value of the image $I$.

Therefore, a natural interval extension of Eq.~(\ref{eq-8a}) is given as:
\begin{equation}\label{eq-8b}
\textbf{PSNR} (\tilde{\mathbf{Y}},\mathbf{Y}) = 10 \cdot \log_{10} \frac{[MAX_I,MAX_I]^2}{\mathbf{MSE}(\tilde{\mathbf{Y}}, \mathbf{Y})},
\end{equation}
when $[MAX_I,MAX_I]$ denotes the degenerate interval obtained by $MAX_I$ and $\mathbf{MSE}(\tilde{\mathbf{Y}}, \mathbf{Y})$ is the interval extension of $MSE(\tilde{Y},Y)$.
%\red{alice 6/10 duvida. $[MAX_I,MAX_I]$ estah estranho aparecer I nos dois lados. um deles nao seria Y-intervalar?}
% OI, VINICIUS. ESTA DÚVIDA ACIMA TAMBÉM FOI RESOLVIDA???.

Finally, it should be noticed that a computation of the MSE between two identical images, the value will be zero and hence the PSNR will be undefined. Moreover, as the main limitation, both metrics relies strictly on numerical comparison, on which  exactly focuses our study in this paper.


\subsection{C-XSC Library} \label{sec:intext}

To make interval results able to contain all possible punctual results, algorithms performing only interval arithmetic have to be designed. In this context, the HWT and its many formulations are extended according to the interval arithmetic, considering the C-XSC interval library~\cite{cxsclink}.

C-XSC is an extensive C++ class library  for SC including a set of basic data types (from intervals to multiple precision complex intervals) whose   high
accuracy computation must be available for some basic arithmetic operations,
mainly the operations that accomplish the summation and (optimize) dot
product. Therefore, some concepts of Mathematics of Computation and
Computational Arithmetic have been incorporated into this system,
such as high accuracy arithmetic, interval mathematics and  automatic numerical verification.
Besides,  C-XSC  is an open source library and available from \cite{cxsclink} adding  additional packages in the source code~\cite{cxsc3}.

Function and operator overloading allow the common mathematical notation of expressions involving interval types. This is also true for (interval) matrix/vector expressions.
Numerical verification methods, also called self-validating methods, are
constructive and they allow to handle uncertain data with mathematical rigor.
The C-XSC library provides support for users to develop efficient
numerical verification methods to obtain  correct and self-validating numerical applications.


In the last years significant improvements have been made in parallelized versions of interval linear system solvers supplied by C-XSC, for more details see~\cite{pblaslink, kolbergfernandesclaudio2008, milani2010}. Once the interval extensions allow different error computations, different compression algorithms can be derived, as shown in Section~\ref{sec:compress}.


% With the summarized background already presented, in the remainder of %the current study the extension of the Haar wavelet transform for the %interval context is obtained. Taking advantage of the interval %arithmetics some formulation simplifications are made in order to %improve the quality of the interval extensions of the transforms. As an application, image compression is also pointed out in the interval context.

\chapter{Discrete Wavelet Transforms}
\section{Haar Wavelet Transform}

\subsection{Decimated Formulation}\label{sec:hwt-decimated}

The Haar basis was proposed in 1910, introduced by the Hungarian mathematician Alfred Haar \cite{haara} in a diferent approach than the one considered by Daubechies in 1988, when she presented her orthogonal basis with compact support for the space of square integrable functions\cite{daubechies1992}, called wavelets. Nevertheless, through her work, the Haar functions started to be seen as a particular case of orthogonal wavelets. Nowadays, the HWT is well stated with some different fast algorithm formulations, been well established for many applications, specially those involving data compression, as considered by JPEG-2000 \cite{JPEG2000}.

\subsubsection{One Dimensional}\label{sec:hwt1d_decimated}

According to the description in \cite{stollnitz1995}, when assuming the decimated formulation of the transform and given an initial vector $C$ with $n$ punctual values, the one-dimensional HWT calculates the averages and differences of each pair of adjacent elements in the input vector $ C $, $(C_{2j-1}, C_{2j}), j=1,...,n/2$, generating therefore two output sets:  one for the scaling (averages) and another for the wavelets (differences) coefficients, both with $n_1= n/2$ points, half size of the original input vector C.

Figure~\ref{fig:DandDS} presents the  $log_2(n)$ level decomposition of the Int-HWT algorithm,  formulated for the decimated case as an extension of the original algorithm presented in~\cite{stollnitz1995}.
The novelty here on the interval code remains in the definition of vectors as interval vectors. Constants are defined as interval quantities and all arithmetics are treated as interval operations. In this sense, in Figure~\ref{fig:DandDS} the interval quantities  $h=( 1/\sqrt{[2;2]}, 1/\sqrt{[2;2]})$ and $g=( 1/\sqrt{[2;2]}, -1/\sqrt{[2;2]})$ are the  normalized interval filters associated to the Int-HWT, representing the interval extension of the standard algorithm. The discrete convolution from $C$ with $h$ generates the averages (scaling coefficients) and  for computing the differences (wavelet coefficients) the filter $g$ is considered.

Since the HWT main feature is to decompose information in many resolution levels, assuming as input the set of the scaling coefficients that contains the $n_1$ previously computed averages, the next level of the decimated HWT produces a second pair of half-sized vectors, one for the new $n_2=n_1/2$ averages and other for the corresponding $n_2=n_1/2$ wavelet coefficients.  This procedure is recursively defined and can be applied until a specific level of coarser resolution is achieved or until a single scalar scaling coefficient is obtained. The whole process of the direct HWT is also called \textbf{decomposition}.

\begin{figure}[ht]
	\centering
    \includegraphics[width=14cm]{Imagens/D_and_DStep.png}
    \caption{1D decimated Int-HWT: $l=log_2(n)$ decomposition levels. Normalized interval filters: $h=( 1/\sqrt{[2,2]}, 1/\sqrt{[2,2]})$ for computing scaling coefficients; $g=( 1/\sqrt{[2,2]}, -1/\sqrt{[2,2]})$ for wavelet coefficients.}
    \label{fig:DandDS}
 \end{figure}


% \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{1D decomposition step.}
%     \label{alg:OriginalDecompositionStep1D}
%     \KwProcedure{DStep($ C[1 .. n] $ : real)}

%         \nl \For{$ \quad i \leftarrow 1 \quad $ \textbf{to} $ \quad n / 2 \quad $}
%         {
%             \nl $ C'[i] \leftarrow (C [2 i - 1] + C [2 i]) / \sqrt{2} $ \;
%             \nl $ C'[i + n / 2] \leftarrow (C [2 i - 1] - C [2 i]) / \sqrt{2} $ \;
%         }

%         \nl $ C \leftarrow C' $

% \end{algorithm}

% \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{1D decomposition process.}
%     \label{alg:OriginalDecomposition1D}
%     \KwProcedure{D($ C[1 .. n] $ : real)}

%     	\nl $ C \leftarrow C / \sqrt{n} $ \;
%         \nl \While{$ \quad n > 1 \quad $}
%         {
%             \nl $ DStep(C[1 .. n]) $ \;
%             \nl $ n \leftarrow n / 2 $ \;
%         }

% \end{algorithm}


Figure \ref{fig:ComparisonIntPonc} shows the comparison between the standard and the interval procedures of the one-dimensional HWT assuming $l=log_2 n$ decomposition levels.
%
\begin{figure}[ht]
    %\centering
    \includegraphics[width=12cm]{Imagens/FiguraA.png}
    \caption{1D decimated HWT algorithms:(top panel) original; (bottom panel) interval extension. Both cases assuming initial data as being punctual values and $log_2 n$ decomposition levels.}
    \label{fig:ComparisonIntPonc}
\end{figure}
%
It is possible to notice in Figure~\ref{fig:ComparisonIntPonc} the care with the type of data being used in the formal parameters in both procedures. The constants must be expressed by intervals too. The instruction $sqrt(float(n))$, square root of punctual floating value, becames $sqrt(interval(n))$, square root of punctual interval. Inside the while loop the interval procedure calls another interval procedure, maintaining the compatibility of the data being calculated. The same type of code adaptation is considered for the rest of the punctual procedures, generating in this way the proposed interval extension.


Using the sets of all wavelet coefficients, resulted from the decomposition process, together with the averages in the lowest level of the direct decimated Int-HWT, it is possible to accurately perform the \textbf{inverse transformation} (also called \textbf{composition}), generating the exact reconstruction of the original vector, which is an important characteristic of the HWT.

The pseudo-code that performs the level composition of  the 1D decimated Int-HWT is represented in Figure~\ref{fig:CandCS}. According to \cite{stollnitz1995}, in the composition process, starting from the coarsest resolution level of the transformation, the original values are restored level by level, combining the wavelet and scaling coefficients from the level immediately below.
Therefore, the decomposition process is completely reversible, allowing the data reconstruction in each level of the transformation, until the end of the process is achieved, when finally the finest resolution level is exactly reconstructed.

\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{Imagens/C_and_CStep.png}
    \caption{Inverse 1D decimated Int-HWT. Composition  process to reconstruct $l=log_2 n$ levels.}
    \label{fig:CandCS}
 \end{figure}

% \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{1D composition step.}
%     \label{alg:OriginalCompositionStep1D}
%     \KwProcedure{CStep($ C[1 .. n] $ : real)}

%     	\nl \For{$ \quad i \leftarrow 1 \quad $ \textbf{to} $ \quad n / 2 \quad $}
%         {
%             \nl $ C'[2 i - 1] \leftarrow (C [i] + C [i + n / 2]) / \sqrt{2} $ \;
%             \nl $ C'[2 i] \leftarrow (C [i] - C [i + n / 2]) / \sqrt{2} $ \;
%         }

%         \nl $ C \leftarrow C' $

% \end{algorithm}

% \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{1D composition process.}
%     \label{alg:OriginalComposition1D}
%     \KwProcedure{C($ C[1 .. n] $ : real)}

%     	\nl $ i \leftarrow 2 $ \;
%         \nl \While{$ \quad i < n \quad $}
%         {
%             \nl $ CStep(C[1 .. 2 i]) $ \;
%             \nl $ i \leftarrow 2 i $ \;
%         }

%         \nl $ C \leftarrow C \sqrt{n} $ \;

% \end{algorithm}

\subsubsection{Two Dimensional}
\label{sec:hwt2d}

The HWT can be extended for two-dimensional vectors. According to the procedure described in \cite{stollnitz1995}, the fast algorithm for the 2D decimated HWT is obtained through the application of the one-dimensional transformation per direction (in all rows of the input matrix and after that, in all columns of the resulting one). In fact, the order how the many levels of the one-dimensional transformation are applied to the two-dimensional data generates different intermediate results and, therefore, distinct algorithms for the 2D transform.

Following what was presented in \cite{stollnitz1995},  the 2D  decimated Int-HWT can be calculated through the standard method, showed in Figure~\ref{fig:SDandND}. In this formulation, the many levels of the one-dimensional transform are applied to all rows.
After the $l=log_2 n$ decomposition levels of the 1D HWT were applied to the entire set of rows of the matrix, the same 1D procedure is applied to the columns to the resulting matrix, as many levels as done for the rows. This completes the 2D decimated transformation, assuming $l=log_2 n$ decomposition levels. Figure \ref{fig:DecompositionMethods} illustrates the application of this algorithm.


% \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{2D standard decomposition (SD).}
%     \label{alg:StandardAlgorithm}
%     \KwProcedure{SD($ C[1..h, 1..w] $ : real)}

%     	\nl \For{ $ \quad row \leftarrow 1 \quad $ \textbf{to} $\quad  h \quad $ }
%         {
%         	\nl $ D(C[row, 1..w]) $ \;
%         }

%         \nl \For{ $ \quad col \leftarrow 1 \quad $ \textbf{to} $\quad  w \quad $ }
%         {
%         	\nl $ D(C[1..h, col]) $ \;
%         }

% \end{algorithm}

\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{Imagens/SD_and_ND.png}
    \caption{2D decimated HWT: Standard and non-standard decomposition processes.}
    \label{fig:SDandND}
 \end{figure}

 \begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{Imagens/Figura2.png}
    \caption{2D decimated HWT with $n\geq 3$ decomposition levels.(left panel)standard algorithm; (right panel) non-standard method.}
    \label{fig:DecompositionMethods}
 \end{figure}

 According to \cite{stollnitz1995} and shown in Figure \ref{fig:SDandND},the non-standard method is another option for the 2D decimated HWT, whose interval extension is also proposed here. In this formulation the operations by rows and columns through out each level of the transform are intercalated. Thereby, every row is decomposed in one level and right after that all columns are also decomposed in one level, completing one level of decomposition for the entire 2D data. In this non standard formulation of the 2D HWT, the resulting scaling coefficients after one decomposition level, which are one fourth of the entire initial data, are then considered as the input for the computation of the next decomposition level of the transform. The three remaining blocks with wavelet coefficients are not further decomposed. Each one also containing one fourth of the original data.  The complete process repeats itself until in the last level just a single scaling coefficient and just one wavelet coefficient for the remaining three sets are obtained. A representation of this formulation is presented in Figure~\ref{fig:DecompositionMethods}(right panel).

%  \begin{algorithm}[H]
% 	\SetKw{KwProcedure}{Procedure}
%     \caption{2D nonstandard decomposition (ND).}
%     \label{alg:NonstandardAlgorithm}
%     \KwProcedure{ND($ C[1..h, 1..w] $ : real)}

%     \nl $ C \leftarrow C / h $ \;
%     \nl \While{$ \quad h > 1 \quad $}
%     {
%       \nl \For{ $ \quad row \leftarrow 1 \quad $ \textbf{to} $\quad  h \quad $ }
%       {
%           \nl $ D(C[row, 1..w]) $ \;
%       }

%       \nl \For{ $ \quad col \leftarrow 1 \quad $ \textbf{to} $\quad  w \quad $ }
%       {
%           \nl $ D(C[1..h, col]) $ \;
%       }
%       \nl $ h \leftarrow h / 2 $
%     }

% \end{algorithm}

  \begin{figure}[ht]
    \centering
    \includegraphics[width=13cm]{Imagens/Inverse_SC_NSC.png}
    \caption{2D decimated HWT - composition process: (left panel) standard formulation; (right panel) non-standard formulation.}
    \label{fig:InverseSCandNSC}
 \end{figure}

  The direct and inverse Int-HWT, Figures \ref{fig:DandDS}, \ref{fig:CandCS}, \ref{fig:SDandND} and \ref{fig:InverseSCandNSC}, can be thought as a convolution of the input data with a pair of filters , $h=(1/2, 1/2)$ and $g=(1/2,-1/2)$, that represent the transform \cite{stollnitz1995}.  The normalized Haar filters are $h=(1/\sqrt{2}, 1/\sqrt{2})$ and $g=(1/\sqrt{2},-1/\sqrt{2})$ and  the normalized transform preserves the energy of the entire set of coefficients obtained through the HWT. Therefore, according to \cite{stollnitz1995}, besides the standard and non-standard algorithms, the HWT can be performed using  normalized and non-normalized filters. Therefore combining the two possible algorithms with these two possible filter choices, there are four ways to compute the decimated 2D Int-HWT available: (i) non-standard, non-normalized; (ii) non-standard, normalized; (iii) standard, non-normalized; and (iv) standard, normalized.

\subsection{Undecimated Formulation}
\label{sec:hwt-undecimated}

Another alternative version for the HWT is the undecimated approach, which avoids the decimation operation after the convolution with the filters has been computed. This undecimated transform has one well established formulation, called the {\it à trous} algorithm, which is considered in many astrophysical and statistical applications \cite{IEEE2007,kozakevicius2013}, specially for been a translation invariant transform.

\subsubsection{One Dimensional}
\label{sec:hwt1d-undecimated}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
According to what is shown in \cite{IEEE2007}, given an initial vector $ C^0 $ with $n_0=2^J$ punctual values, the vector $C^1$ that contains the first decomposition level of the 1D undecimated HWT (à-trous HWT) stores the averages for each pair of adjacent elements $(C^0_j,C^0_{j+1}), j=1,...,n_0$, (assuming $C^0_{n_0+1}=C^0_{0}$). The difference now with respect to the Cascade formulation is that the range of $j$ to select the values $C^0_j$ covers all positions $j=1,...,n_0$. This is exactly what avoids the decimation step and causes $C^1$ to have the same size as $C^0$. The way the wavelet coefficients are computed  also differs from the previous formulation. Now the vector with the differences $D^1$  is computed by $D^1_j=C^0_j -C^1_j, \ j=1,...,n_0$.
%%%%%
The  {\it à trous} HWT is again constructed recursively, until a certain decomposition level $0<l<log_2 n_0$ is achieved. In this sense, the vector of averages $C^j$  at a level $0< j \leq l$ is obtained from the set $C^{j-1}$, the averages at the previous level. Analogously, the vector with the wavelet coefficients is given by $D^j=C^{j-1}-C^j$.  This process is  again called Decomposition or Direct Transformation and it is illustrated in Figure~\ref{fig:DTAtrous} .

%The operations over the last element of the vector are %performed using the first element from the same vector %to form its pair, avoiding the necessity of %extrapolation to deduce the next element after the end %of a vector.

\begin{figure}[ht]
	\vspace{-0.2cm}
    \small
      \begin{tabular}{l l l}
          $C^{0} = [9 \quad 7 \quad 5 \quad 3] \quad \rightarrow $ & $ C^1 = [8 \quad 6 \quad 4 \quad 6] \quad \rightarrow $ & $ C^{2} =  [7 \quad 5 \quad 5 \quad 7] \quad \rightarrow \enskip ... $  \\
          $ $ & $ D^1 = [1 \quad 1 \quad 1 -3] $ & $ D^{2} =  [1 \quad 1 -1 -1] $
      \end{tabular}
      \caption{1D à trous HWT: example with 2 decomposition levels}
    \label{fig:DTAtrous}
    \vspace{-0.3cm}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=9cm]{Imagens/Atrous_D_and_DStep.png}
	\caption{1D à-Trous HWT- Decomposition algorithm.}
    \label{fig:AtrousDecompMethods}
\end{figure}

Summing all vectors of wavelet coefficients resulted by the $n $ decomposition levels together with the averages in the lowest level $n$, it is possible to perform, accurately, the Inverse Transformation,  $ C^0 = C^{n} + \sum_{i=1}^{n} D^{i} $,
also called Composition, which provides the exact reconstruction of the original input vector of data, being an important characteristic of Wavelet Transforms. The composition pseudo code is shown in Figure~\ref{fig:AtrousCompMethod}.

\begin{figure}[ht]
\includegraphics[width=7cm]{Imagens/Atrous_C.png}
	\caption{1D à-Trous HWT- Composition algorithm.}
    \label{fig:AtrousCompMethod}
\end{figure}

\subsubsection{Two Dimensional}
\label{sec:hwt2d-undecimated}

According to \cite{IEEE2007}, the algorithm for the 2D à trous HWT is again obtained through the application of the one-dimensional transformation in all rows and columns of the input matrix, as done for the decimated version of the HWT. Analogously, Standard and Non-Standard procedures can be designed according to the order how the 1D à trous transforms are applied to the 2D data.

\begin{figure}[h!]
\includegraphics[width=15cm]{Imagens/HaarAtrousStandard.png}
	\caption{2D à trous HWT- standard algorithm for matrix decomposition.}
    \label{fig:AtrousStandard}
\end{figure}

\begin{figure}[h!]

\includegraphics[width=10cm]{Imagens/HaarAtrousStandardPseudo.png}

	\caption{2D à trous HWT: Pseudo code for the standard matrix decomposition.}
    \label{fig:AtrousStandardPseudo}

\end{figure}

The undecimated standard decomposition for the 1D Int-HWT, based on the Standard decimated decomposition, is performed by executing the 1D à trous Int-HWT for each row of the input matrix. This process is called Row~À-Trous~Transform~(\textit{RAT}), characterizing this procedure as a horizontal transformation. Considering $n$ decomposition levels, the RAT procedure is repeated $n$ times, generating matrices $ C^1_R $ ... $ C^n_R $ and $ D^1_R $ ... $ D^n_R $. After decomposing all rows from the input matrix, the decomposition of all its columns is performed, called Column~À-Trous~Transform~(\textit{CAT}), characterizing a vertical transformation and generating matrices $ C^{n+1}_C $ ... $ C^{n*2}_C $ and $ D^{n+1}_C $ ... $ D^{n*2}_C $.

The matrices $ C^j_R $ and $ C^j_C $  store scaling coefficients, while $ D^j_R $ and $ D^j_C $ store wavelet coefficients. A visual representation of the process is shown in Figure~\ref{fig:AtrousStandard}, and its pseudo algorithm is shown in Figure~\ref{fig:AtrousStandardPseudo}.

\begin{figure}[h!]

	\includegraphics[width=11cm]{Imagens/HaarAtrousNonStandardPseudo.png}

	\caption{Pseudo code for the Non Standard matrix decomposition.}
    \label{fig:AtrousNonStandardPseudo}

\end{figure}

\begin{figure}[h!]

	\includegraphics[width=15cm]{Imagens/HaarAtrousNonStandard.png}

	\caption{Non Standard algorithm for matrix decomposition.}
    \label{fig:AtrousNonStandard}

\end{figure}

The undecimated non-standard decomposition, based on the original non-standard algorithm from the Cascade approach, is performed by intercalating both \textit{RAT} and \textit{CAT} operations, extracting vertical and horizontal details for each level of decomposition, as shown in Figure~\ref{fig:AtrousNonStandard}. For each level $ j $ of decomposition the algorithm performs a \textit{RAT} step, generating matrices $ C^j_R $ and $ D^j_R $, and right after a \textit{CAT} is performed, generating matrices $ C^j_C $ and $ D^j_C $. Both $ C^j_R $ and $ C^j_C $ store scalar coefficients, while $ D^j_R $ and $ D^j_C $ hold wavelet coefficients, and those four matrices represent one level full decomposed. The À-trous Non Standard algorithm is shown in Figure~\ref{fig:AtrousStandardPseudo}

%\todo[inline]{Depois que falares dos algoritmos atrous 1d e 2d, podes entao entrar na questao da normalizacao, pois tanto no caso do algoritmo de cascata, quanto no algoritmo atrous, tu conseguiste as simplificaçoes que vao otimizar os codigos intervalares. acho mais prudente chamar de simplificação ao invés de otimizacao. \\
%O parágrafo abaixo é um bom link entre seções. então, podes aproveitá-lo para a conexão entre a seção anterior e a próxima}

The same idea from the one-dimensional transform can be applied for both standard and non-standard sets of data to perform an inverse process, generating the input matrix. By using all matrices of wavelet coefficients resulted by the process of decomposition together with the averages in the lowest level of decomposition, it is possible to reconstruct, accurately. $MatrixComposition$ generates the exact reconstruction of the original input matrix of data. The algorithm describing its execution is shown in Figure~\ref{fig:ATrousMatrixComposition}.

\begin{figure}[h!]
	\includegraphics[width=11cm]{Imagens/ATrousMatrixComposition.png}
	\caption{Pseudo code for À-trous matrix composition.}
    \label{fig:ATrousMatrixComposition}

\end{figure}

\section{Daubechies Wavelet Transform}

\subsection{One Dimensional}\label{sec:1DDaWT}

According to the description in \cite{DAUBTHESIS}, when assuming the original formulation of the transform and given an initial vector $C$ with $n$ punctual values, the one-dimensional DaWT calculate scaling and wavelet coefficients of each pair of adjacent elements in the input vector $C$, $(C_{2j-1}, C_{2j}), j=1,...,n/2$, generating therefore two output sets: a vector containing the scaling and a second vector with wavelet coefficients, both with $n_1= n/2$ points, half size of the original input vector $C$.

Figure~\ref{fig:DaubDandDS} presents the $log_2(n)$ level decomposition of the Int-DaWT algorithm,  formulated for the decimated case as an extension of the original algorithm presented in~\cite{DAUBTHESIS}. The novelty here on the interval code remains in the definition of vectors as interval vectors. Constants are defined as interval quantities and all arithmetics are treated as interval operations. In this sense, in Figure~\ref{fig:DaubDandDS}, the interval quantities  $h$ and $g$, shown in Figure~\ref{fig:IntDaWHFilters}, are the  normalized interval filters associated to the Int-DaWT, representing the interval extension of the original algorithm. The discrete convolution from $C$ with $h$ generates the next set of scaling coefficients and for computing the wavelet coefficients the filter $g$ is considered.

\begin{figure}[h!]

\begin{center}
\Large
$h = (
\quad \frac{1+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3-\sqrt{3}}{4\sqrt{2}},
\quad \frac{1-\sqrt{3}}{4\sqrt{2}}
\quad )$
\end{center}

\begin{center}
\Large
$g = (
\quad \frac{1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{-3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1-\sqrt{3}}{4\sqrt{2}}
\quad )$
\end{center}

\caption{Scaling and Wavelet filters for the direct DaWT}
\label{fig:DaubFilters}
\end{figure}

\begin{figure}[h!]

\begin{center}
\Large
$h' = (
\quad \frac{[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[3;3]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
\quad )$
\end{center}

\begin{center}
\Large
$g' = (
\quad \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{-[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{-[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
\quad )$
\end{center}

\caption{Interval extension of the Scaling and Wavelet filters for the direct DaWT}
\label{fig:IntDaWHFilters}
\end{figure}

\begin{figure}[h!]

\begin{center}
\Large
$ih = (
\quad \frac{3-\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{1+\sqrt{3}}{4\sqrt{2}},
\quad \frac{1-\sqrt{3}}{4\sqrt{2}}
\quad )$
\end{center}

\begin{center}
\Large
$ig = (
\quad \frac{1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1-\sqrt{3}}{4\sqrt{2}},
\quad \frac{3+\sqrt{3}}{4\sqrt{2}},
\quad \frac{-1+\sqrt{3}}{4\sqrt{2}}
\quad )$
\end{center}

\caption{Scaling and Wavelet filters for the inverse DaWT}
\label{fig:InvDaubFilters}
\end{figure}

\begin{figure}[h!]

\begin{center}
\Large
$ih' = (
\quad \frac{[3;3]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
\quad )$
\end{center}

\begin{center}
\Large
$ig' = (
\enskip \frac{[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{-[1;1]-\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{[3;3]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}},
\quad \frac{-[1;1]+\sqrt{[3;3]}}{[4;4]\sqrt{[2;2]}}
\quad )$
\end{center}

\caption{Interval extension of the Scaling and Wavelet filters for the inverse DaWT}
\end{figure}

Since the DaWT main feature is to decompose information in many resolution levels, assuming as input the set of the scaling coefficients that contains the $n_1$ previously computed averages, the next decomposition level produces a second pair of half-sized vectors, one for the new $n_2=n_1/2$ scaling coefficients and other for the corresponding $n_2=n_1/2$ wavelet coefficients. This procedure is recursively defined and can be applied until a specific level of coarser resolution is achieved or until two scaling coefficients are obtained.

\begin{figure}[h!]
    \includegraphics[width=7.2cm]{Imagens/Daub_D.png}
    
    \includegraphics[width=14cm]{Imagens/Daub_DStep.png}
    \caption{1D DaWT decomposition procedure.}
    \label{fig:DaubDandDS}
 \end{figure}

Using the sets of all wavelet coefficients, resulted from the decomposition process, together with the scaling coefficients in the lowest level of the decomposed data, it is possible to perform the inverse transformation, also known as composition), reconstructing the original vector, which is an important characteristic of DWTs.

The pseudo-code that performs the level composition of the 1D DaWT is represented in Figure~\ref{fig:DaubCandCS}. According to \cite{DAUBTHESIS}, in the composition process, starting from the coarsest resolution level of the transformation, the original values are restored level by level, combining the wavelet and scaling coefficients from the level immediately below. Therefore, the decomposition process is completely reversible, allowing the data reconstruction in each level of the transformation, until the end of the process is achieved, when finally the finest resolution level is exactly reconstructed.

\begin{figure}[h!]
    \includegraphics[width=7.1cm]{Imagens/Daub_C.png}
    
    \includegraphics[width=14cm]{Imagens/Daub_CStep.png}
    \caption{Inverse 1D DaWT, Composition process to reconstruct $l=log_2 n$ levels.}
    \label{fig:DaubCandCS}
 \end{figure}

\subsection{Two Dimensional}\label{sec:2DDaWT}

The DaWT can be extended for two-dimensional vectors by using the same concept presented for the HWT in Section~\ref{sec:hwt2d}. According to the procedure described in \cite{DAUBTHESIS}, the fast algorithm for the 2D DaWT is obtained through the application of the one-dimensional transformation per direction (in all rows of the input matrix and after that, in all columns of the resulting one). In fact, the order how the many levels of the one-dimensional transformation are applied to the two-dimensional data generates different intermediate results and, therefore, distinct algorithms for the 2D transform.

Following what was presented in \cite{DAUBTHESIS}, the 2D DaWT can be calculated through the standard method, showed in Figure~\ref{fig:SDandND}. In this formulation, the many levels of the one-dimensional transform are applied to all rows. After the $l=log_2 n$ decomposition levels of the 1D DaWT were applied to the entire set of rows of the matrix, the same 1D procedure is applied to the columns of the resulting matrix, as many levels as done for the rows. This completes the 2D transformation, assuming $l=log_2 n$ decomposition levels. Figure \ref{fig:DecompositionMethods} illustrates the application of this algorithm.

According to \cite{DAUBTHESIS} and shown in Figure \ref{fig:SDandND},the non-standard method is another option for the 2D DaWT. In this formulation the operations by rows and columns through out each level of the transform are intercalated. Thereby, every row is decomposed in one level and right after that all columns are also decomposed in one level, completing one level of decomposition for the entire 2D data. In this non standard formulation of the 2D DaWT, the resulting scaling coefficients after one decomposition level, which are one fourth of the entire initial data, are then considered as the input for the computation of the next decomposition level of the transform. The three remaining blocks with wavelet coefficients are not further decomposed. Each one also containing one fourth of the original data.  The complete process repeats itself until in the last level just a single scaling coefficient and just one wavelet coefficient for the remaining three sets are obtained. A representation of this formulation is presented in Figure~\ref{fig:DecompositionMethods}(right panel).

The direct and inverse of the DaWT, Figures \ref{fig:DaubDandDS}, \ref{fig:DaubCandCS}, \ref{fig:SDandND} and \ref{fig:InverseSCandNSC}, can be thought as a convolution of the input data with two normalized pairs of filters, $h$ and $g$ shown in Figure~\ref{fig:DaubFilters}, that represent the transform. The normalized transform preserves the energy of the entire set of coefficients obtained through the HWT. Therefore, according to \cite{DAUBTHESIS}, besides the standard and non-standard algorithms, the HWT can be performed using  normalized and non-normalized filters. Thus combining the two possible algorithms with these two possible filter choices, there are four ways to compute the decimated 2D DaWT available: (i) non-standard, non-normalized; (ii) non-standard, normalized; (iii) standard, non-normalized; and (iv) standard, normalized.

\chapter{Developed Optimal Formulations}
\label{sec:optintform}

When the normalized filters are considered in the original algorithms of the HWT  \cite{stollnitz1995, IEEE2007},  divisions by  $ \sqrt{2} $  are executed in all iterations within the decomposition.
Once  $ \sqrt{2} $   is not a computable value, each level of the decomposition or composition adds a certain degree of error into the data. The error generated in each iteration is then propagated through all levels until the end of the procedure.

The solution proposed to avoid this issue for the interval extension is based on performing algebraic simplifications to eliminate the computation of these non computable values, $2^{j/2}$ , whenever possible, reducing the error involved in the process. Therefore the developed interval procedures produce more trustworthy results in comparison with the original algorithms analyzed here \cite{stollnitz1995} and \cite{kozakevicius2013}.

 The original algorithms and their interval versions developed here are compared. Assuming  the same input data is analyzed with all different formulations of the HWT:  the Cascade (decimated) and à-trous (undecimated) algorithms for both normalized and non-normalized approaches.

 The basic idea for the algebraic simplifications is based on using the non-normalized transform, which introduces no extra computation errors for the HWT. By shifting the normalization step to the end of the transform, all operations with $ \sqrt{2} $ are applied only once, decreasing the error produced by its computation in each level of the decomposition.

\section{Haar Wavelet Transform}

\subsection{Decimated Formulation}

\subsubsection{One Dimensional}
\label{subsec:1DCascadeOpt}

 The normalized decomposition procedure for the decimated version of the Int-HWT, either 1D or 2D, is executed in the following order: first a non-normalized decomposition is made, as indicated by the pseudo-code in Figure~\ref{fig:Fig4}; after this stage the normalization of all coefficients is performed, multiplying all of them by the normalization factor $ 2^{-j/2} $, where $ j $ is the resolution level of the coefficients through out the decomposition.
 
 %\todo{De repente escrever como algoritmo? Pilla}
 % É uma boa idéia, vou deixar isso anotado aqui!

Depending on $ j $, the computation of $ \sqrt{2} $ may not be necessary, avoiding  any computation error in this case. This process is illustrated in Figure \ref{fig:Fig4}, assuming 2 levels of the 1D HWT and considering the same example as presented in \cite{stollnitz1995}.

 \begin{figure}[ht]
    \centering
    $[9 \quad 7 \quad 3 \quad 5] \rightarrow [6 \quad 2 \quad 1 \quad -1] \rightarrow [6 \quad 2 \quad \frac{1}{\sqrt{2}} \quad \frac{-1}{\sqrt{2}}]$

    \vspace{0.2cm}

    $ \textbf{Input} \quad \quad \rightarrow \textbf{Decomposition} \rightarrow \textbf{Normalization} $
    \caption{Example of the one-dimensional optimization.}
    \label{fig:Fig4}
 \end{figure}

Looking at the original decomposition procedure, shown in Figure~\ref{fig:DandDS}, the complexity of $DecompositionStep$ is $\BigO(n + 1)$, and for the main $ decomposition $ algorithm the complexity is $\BigO(n + \log(n) * (n + 2) + 1)$. By the proposed simplification they are reduced to $\BigO(n)$ and $\BigO(n + n\log(n))$, respectively. By performing the non-normalized decomposition there is no need to execute the normalization step at line 1 of the original $decomposition$ algorithm, neither the divisions by $\sqrt{2}$ at its step procedure. Therefore, overall complexity for non-normalized transformation of a $n$ sized vector is $\BigO(n\log(n))$, representing the pseudo-code shown in Figure~\ref{fig:NewDandDS}.

\begin{figure}[ht]
    %\centering
    \includegraphics[width=13.6cm]{Imagens/New_D_and_DStep.png}
    \caption{Int-HWT, decimated version: non-normalized Decomposition process and its step procedure.}
    \label{fig:NewDandDS}
 \end{figure}

The normalization procedure, presented in Figure~\ref{fig:1DNorm_and_Denorm}, performs the normalization step of all coefficients after their convolution using filters. Since it is executed once on every coefficient of a $n$ sized vector, its complexity can be $\BigO(n)$. By performing a non-normalized transform and the normalization procedure afterwards, the complexity is $\BigO(n\log(n) + n)$, which is the same complexity from the normalized transformation. The goal is therefore the gain on exactitude obtained by avoiding high number of divisions using $\sqrt{2}$ from the original $DecompositionStep$ algorithm, as shown in Figure~\ref{fig:test1-3}.

\begin{figure}[ht]
	%\centering
    \includegraphics[width=12.4cm]{Imagens/Norm_and_Denorm.png}
    \caption{Int-HWT, decimated version: on dimensional normalization procedure.}
    \label{fig:1DNorm_and_Denorm}
 \end{figure}

The normalized composition procedure follows the same basic idea to reduce computation of $\sqrt{2}$, it performs the non-normalized inverse transform, but this time a denormalization step, shown in Figure~\ref{fig:1DNorm_and_Denorm}, is executed before the transform begins. The original algorithms for composition and its step procedure, Figure~\ref{fig:CandCS}, behave similarly to decomposition. The complexity of the $Composition$ procedure is $\BigO(n\log(n) + n)$ if the normalized formulation is considered, otherwise it is $\BigO(n\log(n))$. The denormalization step, also shown in Figure~\ref{fig:1DNorm_and_Denorm}, is almost the same $Normalization$ algorithm, whose complexity is $\BigO(n)$. The difference is at line 6, where it is dividing each coefficient by the normalization factor, preparing the data to be transformed. Therefore the optimal composition has the complexity from the original version, $\BigO(n\log(n) + n)$, but its results are more exact due to the normalization step, avoiding the calculation of $\sqrt{2}$ on every iteration.



\subsubsection{Two Dimensional}

\label{subsec:cascade2D_Opt_Haar}
%
The simplifications implemented for the 2D int-HWT consider the same principle presented in the 1D case, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{-j/2}$ normalization factor, where $j$ is the corresponding level. To compose or decompose a matrix the 1D transform is applied in all rows and in all columns of the input matrix.

However, according to \cite{stollnitz1995}, there are two main algorithms for composition and decomposition of matrices, know as Standard and Non-Standard procedures, what suggests distinct normalization procedures for both approaches. During the study of the original algorithms~\cite{stollnitz1995}, patterns of normalization factors were recognized. These patterns were analysed and employed in the development of the normalization procedures for both algorithms, assuming the decimated version of the HWT.

These patterns are illustrated in Figure~\ref{fig:2DNormFactors}, where 3 decomposition levels are presented for 8x8 matrices. The parameters $ j' $ and $ j'' $ indicate the  normalization levels. The rule to calculate these normalization factors is described in the equation:
\begin{center}
	\LARGE $ 2^{\frac{-(j' + j'')}{2}} ,$
\end{center}
where $ 0 \leq j' , j'' \leq (\log_{2}n) -1 $ and $ n $ indicates the matrix order.
\begin{figure}[ht]
  	\centering
  	\includegraphics[width=13.6cm]{Imagens/NormFactors.png}
  	\caption{Int-HWT, decimated version: (a) standard, (b) non-standard normalization patterns and (c) rule for normalization factors.}
	\label{fig:2DNormFactors}
\end{figure}

By analyzing the original standard algorithm, Figure~\ref{fig:SDandND}, it depends on the original normalized decomposition procedure, whose complexity is $\BigO(n\log(n) + n)$, executing it for every row and column of the input matrix. Considering a $n$x$n$ matrix the complexity of the original standard algorithm is $\BigO(n(n\log(n) + n) + n(n\log(n) + n))$, therefore $\BigO(2n^2\log(n) + 2n^2)$. Performing the non-normalized decomposition algorithm, discussed in Section~\ref{subsec:1DCascadeOpt}, it reduces the calculations by avoiding intermediate normalization steps.

\begin{figure}[ht]
  	\includegraphics[width=10cm]{Imagens/StandardNormalization.png}
  	\caption{2D decimated Int-HWT: Normalization step for the standard decomposition process.}
	\label{fig:StandardNormAlg}
\end{figure}

The non-normalized standard decomposition is performed with $\BigO(2n^2\log(n))$, and the last step is to normalize the results using the $StandardNormalization$ algorithm, presented in Figure~\ref{fig:StandardNormAlg}, and since it is operating one time on each coefficient of the input matrix its complexity can be expressed by $\BigO(n^2)$. The complexity of performing the non-normalized standard decomposition and the standard normalization step can be expressed by $\BigO(2n^2\log(n) + n^2)$, which is faster than the original normalized procedure by avoiding a second iteration of $\BigO(n^2)$ on the input matrix.

The original normalized standard composition, presented in Figure~\ref{fig:InverseSCandNSC}, can be analyzed in the same way as its decomposition procedure since it performs the same number of operations but produces an approximation of the input matrix used for decomposition. The original version is executed in $\BigO(2n^2\log(n) + 2n^2)$ time, while its optimized version operates in $\BigO(2n^2\log(n) + n^2)$.

Looking at the original non-standard algorithm, Figure~\ref{fig:SDandND}, it is executing a intermediate normalization at line 1, dividing each coefficient by the order of the input matrix, which configures a $\BigO(n^2)$ operation. Considering a quadratic matrix of order $n$, the rest of the algorithm operates in a $while$ loop which is executed $\log_2(n)$ times. For each $while$ loop there are two $for$ loops also executing $\log_2(n)$ times, each one performing the $DecompositionStep$ algorithm ($\BigO(n)$) on portions of the input matrix, depending on the level of transformation. The original non-standard decomposition is executed in $\BigO(n^2 + \log(\log(n) + \log(n)))$ time.

\begin{figure}[ht]
  	\includegraphics[width=8.6cm]{Imagens/NonStandardNormalization.png}
  	\caption{Int-HWT, decimated version: Normalization step for the non-standard decomposition process.}
	\label{fig:NonStandardNormAlg}
\end{figure}

The non-normalized non-standard decomposition ( $\BigO(\log(\log(n) + \log(n)))$) does not need the normalization step,  and its $Non-StandardNormalization$ algorithm, shown in Figure~\ref{fig:NonStandardNormAlg}, is performed with complexity $\BigO(n^2)$. The overall complexity of these procedures is $\BigO(n^2 + \log(\log(n) + \log(n)))$, which is the same from the original normalized non-standard algorithm. The gain of time presented in Figure~\ref{fig:test2S-NS} was obtained performing simple divisions by $2$ which are faster than using $\sqrt{2}$ as divisor when analysing processor microinstructions.

\subsection{Undecimated Formulation}


The simplifications implemented for the 2D int-HWT consider the same principle presented in the 1D case, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{-j/2}$ normalization factor, where $j$ is the corresponding level. To compose or decompose a matrix the 1D transform is applied in all rows and in all columns of the input matrix.

However, according to what was discussed in Section~\ref{sec:hwt2d}, there are two algorithms for composition and decomposition of matrices, what suggests distinct normalization procedures for both approaches. During the study of the original algorithms~\cite{stollnitz1995}, a pattern of normalization factors was recognized. These patterns were analyzed and employed in the development of the normalization procedures for both standard and non-standard algorithms, assuming the decimated version of the HWT.

These patterns are illustrated in Figure~\ref{fig:2DNormFactors}, where 3 decomposition levels are presented for 8x8 matrices. The parameters $ j' $ and $ j'' $ indicate the  normalization levels. The rule to calculate these normalization factors is described in the equation:
\begin{center}
	\large $ 2^{-(j' + j'')/2} ,$
\end{center}
where $ 0 \leq j' , j'' \leq (\log_{2}n) -1 $ and $ n $ indicates the matrix order.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\red{alice 12/10/2015 Pessoal: parei aqui. acho que é melhor o vinicius mexer agora, limpar as coisas que ficaram para tras ao longo do trabalho e depois entao, mexemos de novo no texto. Acho que precisaremos enxugar a discussao. está ficando longo e repetitivo. acho que devemos diminuir a parte inicial que está muito similar como que ja foi publicado e dar mais enfoque para a parte das complexidades e validacao em relacao à aplicacao. só que a meu ver, isso só pode ser feito, depois que chegarmos ao final dessa versao preliminar. nao sei quantas paginas ao certo ja temos, porque há ainda muitos comentarios e recados no texto. minha sugestão é que agora dvemos trabalhar para gerarmos uma versao limpa, daí repensamos o que fazer, dependendo da restricao de paginas da revista. alias, quantas sao???????? ate breve. alice}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



By analyzing the original standard algorithm, Figure~\ref{fig:SDandND}, it depends on the original normalized decomposition procedure, whose complexity is $\BigO(n\log(n) + n)$, executing it for every row and column of the input matrix. Considering a $n$x$n$ matrix the complexity of the original standard algorithm is $\BigO(n(n\log(n) + n) + n(n\log(n) + n))$, therefore $\BigO(2n^2\log(n) + 2n^2)$. Performing the non-normalized decomposition algorithm, discussed in Section~\ref{subsec:1DCascadeOpt}, it reduces the calculations by avoiding intermediate normalization steps.

The non-normalized standard decomposition is performed with $\BigO(2n^2\log(n))$, and the last step is to normalize the results using the $StandardNormalization$ algorithm, presented in Figure~\ref{fig:StandardNormAlg}, and since it is operating one time on each coefficient of the input matrix its complexity can be expressed by $\BigO(n^2)$. The complexity of performing the non-normalized standard decomposition and the standard normalization step can be expressed by $\BigO(2n^2\log(n) + n^2)$, which is faster than the original normalized procedure by avoiding a second iteration of $\BigO(n^2)$ on the input matrix.

The original normalized standard composition, presented in Figure~\ref{fig:InverseSCandNSC}, can be analyzed in the same way as its decomposition procedure since it performs the same number of operations but produces an approximation of the input matrix used for decomposition. The original version is executed in $\BigO(2n^2\log(n) + 2n^2)$ time, while its optimized version operates in $\BigO(2n^2\log(n) + n^2)$.

Looking at the original non-standard algorithm, Figure~\ref{fig:SDandND}, it is executing a intermediate normalization at line 1, dividing each coefficient by the order of the input matrix, which configures a $\BigO(n^2)$ operation. Considering a quadratic matrix of order $n$, the rest of the algorithm operates in a $while$ loop which is executed $\log_2(n)$ times. For each $while$ loop there are two $for$ loops also executing $\log_2(n)$ times, each one performing the $DecompositionStep$ algorithm ($\BigO(n)$) on portions of the input matrix, depending on the level of transformation. The original non-standard decomposition is executed in $\BigO(n^2 + \log(\log(n) + \log(n)))$ time.



The non-normalized non-standard decomposition ( $\BigO(\log(\log(n) + \log(n)))$) does not need the normalization step,  and its $Non-StandardNormalization$ algorithm, shown in Figure~\ref{fig:NonStandardNormAlg}, is performed with complexity $\BigO(n^2)$. The overall complexity of these procedures is $\BigO(n^2 + \log(\log(n) + \log(n)))$, which is the same from the original normalized non-standard algorithm. The gain of time presented in Figure~\ref{fig:test2S-NS} was obtained performing simple divisions by $2$ which is faster than using $\sqrt{2}$ as divisor.

\subsubsection{One Dimensional}

The normalized decomposition procedure for the undecimated version of the Int-HWT, either one-dimensional or two-dimensional, is executed in the same order from the optimization developed for the decimated version: first a non-normalized decomposition is made, as indicated in Figure~\ref{fig:DTAtrous}; after this stage the normalization of all coefficients is performed, multiplying all of them by the normalization factor $ 2^{j/2} $, where $ j $ is the resolution level of the coefficients through out the decomposition.

The complexities of both $ DecompositionStep $ and $ Decomposition $ procedures are  $\BigO(2n + 2)$ and $\BigO(Levels.(2n + 3))$, respectively. Both are part of the original decomposition procedure, shown in Figure~\ref{fig:AtrousDecompMethods}. As in the previous cases, by performing a non-normalized decomposition there is no need to execute divisions by $\sqrt{2}$ in the step procedure, pseudo-code shown in Figure~\ref{fig:NewAtrousDS}, reducing the corresponding complexities to $\BigO(2n)$ and $\BigO(2n.Levels)$ (the same complexity as the original normalized transform).

\begin{figure}[ht]
  	\includegraphics[width=10cm]{Imagens/Atrous_New_DStep.png}
  	\caption{2D undecimated Int-HWT: non-normalized decomposition step.}
	\label{fig:NewAtrousDS}
\end{figure}

The normalization procedure, presented in Figure~\ref{fig:Atrous1DNormalization}, performs the normalization step of all coefficients after the transform. In contrast with the decimated version, the undecimated approach deals with a set of vectors as result of the transform, and each resultant vector need to be normalized individually. The normalization procedure multiplies each scalar coefficient by its normalization factor $ 2^{i/2} $ for every resultant vector from the transform, configuring $ \BigO(n.Levels) $ from line 1 to 5 of the pseudo-code. With the new set of scalar values all wavelet coefficients are then recalculated, configuring $ \BigO(Levels.(n + 3) + 2) $ from line 6 to 16, which can be simplified to $ \BigO(n.Levels) $. The overall complexity of the procedure is $ \BigO(2n.Levels) $.

\begin{figure}[ht]
  	\includegraphics[width=9cm]{Imagens/Atrous1DNormalization.png}
  	\caption{1D undecimated Int-HWT: normalization procedure.}
	\label{fig:Atrous1DNormalization}
\end{figure}

By performing a non-normalized vector transform and the normalization procedure, the complexity is $\BigO(4n.Levels)$, which is more expensive than the original normalized formulation. The gain obtained with the new implementation is on the exactitude of results by avoiding high number of divisions using $\sqrt{2}$ from the original $DecompositionStep$ algorithm, as shown in Figure~\ref{fig:test1-3}.

The composition process, shown in Figure~\ref{fig:AtrousCompMethod}, starts with a copy of all scalar coefficients from the lowest level summing it with all wavelet vectors produced during decomposition. The complexity of this procedure is $ \BigO(n.Levels) $ and can be used for both normalized and non-normalized transformations, there is no need for optimization.

\subsubsection{Two Dimension}

As done for the decimated case, the undecimated transform, preformed by the à-trous algorithm, considers the same principle presented in the one-dimensional optimization, i.e., the same strategy of multiplying the transformed values by the corresponding $2^{j/2}$ normalization factor, where $j$ is the corresponding level. To decompose a data matrix the two-dimensional implementation creates a set of matrices carrying scalar coefficients and another set of matrices to store wavelet coefficients. The procedure is expensive on both performing and storing the results. Applying the same idea used in previous implementations, it is possible to reduce the error involved in the process.

In Figure~\ref{fig:AtrousStandardPseudo} the original standard procedure is presented. It performs the decomposition of every line and level of a $n$x$m$ matrix, configuring a complexity of $ \BigO(n.2m.Levels) $.  The analog procedure for all columns of the same matrix has $\BigO(m.2n.Levels)$. The entire procedure is expressed by $\BigO(n.2m.Levels + m.2n.Levels)$, which can be simplified to $\BigO(4n.m.Levels)$. The non-standard procedure executes the same amount of calculations, but intercalating the decomposition of rows and columns. Both standard and non-standard algorithms can be used in conjunction with $DecompositionStep$ shown in Figure~\ref{fig:NewDandDS}, so the complexity of executing the normalized and non-normalized are the same.

The next step in order to normalize the transformed values is to perform the $A\-TrousMatrixNormalization$ procedure, shown in Figure~\ref{fig:AtrousMatrixNormalization}, which is executed in $\BigO(4n.m.Levels)$. Therefore, the optimized decomposition procedure developed costs $\BigO(8n.m.Levels)$, twice the cost of the original normalized formulation, but tests indicate that our approach results in more exact values as shown in Figure~\ref{fig:test4S-NS}.

\begin{figure}[ht]
  	\includegraphics[width=11.5cm]{Imagens/ATrousMatrixNormalization.png}
  	\caption{2D undecimated Int-HWT: normalization procedure.}
	\label{fig:AtrousMatrixNormalization}
\end{figure}

\section{Interval Compression}
\label{sec:compress}

The main goal of compression procedures is to express an initial data set with the smaller amount of points as possible, this task can be done either with or without loss of information~\cite{stollnitz1995},~\cite{kozakevicius2014}. In the wavelet context, the wavelet expansion of the initial data is analysed in order to decide which wavelet coefficients are more significant then others. Therefore, keeping only the most significant ones, the truncated series represents the compressed(or filtered) data. In~\cite{perin2013} ECG signals were analysed assuming adaptive compression techniques.

In many signal analysis applications, the significant wavelet coefficients are then used  to draw further conclusions about the original data, as for example in~\cite{mohammad2013}, where micro-calcifications in mammograms were detected based on the truncated wavelet representation of the images.

The significance of the coefficients is judged by a threshold value, which can be obtained based on many types of heuristics, linear as the Universal threshold proposed by~\cite{donoho1995} or adaptive as proposed in \cite{bayer2010}. For a review about the many possibilities for choosing threshold values see  \cite{kozakevicius2014}.

By ignoring (and excluding) non significant wavelet coefficients from the wavelet expansion, this strategy turns  the method into a lossy compression procedure. This procedure is called hard thresholding, according to \cite{donoho1995}. The result is an approximation of the original function.  Figure~\ref{fig:compression}, wich can be found in \cite{stollnitz1995}, shows a sequence of approximations, generated by varying the compression rate $s/N$, where $s$ is the number of significant coefficients, and $N$ the total amount of points in the initial data representation.

%  \begin{algorithm}[H]
%     \caption{Punctual compression method.}
%     \label{alg:realcompression}
%     \KwData{$ C[0...n] $ : real}
%     $ i \leftarrow 0 $ \;
%     \For{$ i < n $}
%     {
%         \If{$ |C[i]| < \tau $}
%         {
%             $ C[i] = 0 $ \;
%         }
%         $ i = i + 1 $ \;
%     }
% \end{algorithm}

\begin{figure}[ht]
    \centering
    \includegraphics[width=13.5cm]{Imagens/compression.png}
    \caption{Approximation of functions after compression.}
    \label{fig:compression}
\end{figure}

%\todo[inline]{2) !!!!!!!!!!!!!cuidado!!!!!!!!!!!esta figura 4 está igual ao que foi feito no paper a primer... sugiro pegar uma curva seno(2pix) em [0,1] + uma pequena perturbacao, pode ser uma funcao gausseana centrada no 0.5, que varia de .045 a 0.55. daí, coloque o numero de pontos muito maior do que 16. podes colocar 1024 pontos. daí sim, compare a soft decision e a hard decision e mostre para cada gráfico a taxa de compressao e o erro (distancia euclidiana entre as curvas) daí sim, a análise fica completa e a gente aproveita tudo o que afigura tem pra dar. coloque também as marcacoes das escalas no eixo ox e no oy.}

The hard thresholding is trivial for punctual data, but it can not be applied in the same way to interval information. The punctual algorithm for compression uses a real value $ \tau $ as threshold VALUE, and the decision of which details must be ignored is a set of simple punctual comparisons.

%\todo[inline]{(((!!!!!!!!!!!!ESTA PARTE  SOBRE A COMPRESSAO INTERVALAR É NOVA!!!!!!!!!!!!!!!!EXCELENTE!!!!!!!!!!!MAIS ALGUÉM JÁ FEZ ISSO??? AQUI É PRECISO FAZER UMA BUSCA PARA SABER SE MAIS ALGUÉM TOCOU NESTA QUESTÃO ANTES...SE JÁ FOI FEITO, TEMOS QUE FAZER A CITACAO, E APONTAR DIFERENCAS OU SIMILARIDADES ENTRE O NOSSO E O ANTERIORMENTE PUBLICADO!)\\

%CASO ISSO SEJA INÉDITO...ESSE PEIXE TEM QUE SER VENDIDO LÁ NA INTRODUCAO))))))))))))))))}



When treating with interval data, $ \tau $ turn to be also an interval, carrying the error in the threshold calculus. This way the compress decision can not be evaluated with the same punctual comparison as before~\cite{Moore79}. Two solutions were developed to manage the decision in this interval extension, the first was named as Hard Decision and the second as Soft Decision, inspired by the nomenclature considerd by Donoho for its threshold operators.

\begin{figure}[ht]
    \centering
    \includegraphics[width=13.5cm]{Imagens/C_SDC_HDC.png}
    \caption{Hard Thresholding, Hard Decision and Soft Decision compress procedures.}
    \label{fig:C_SDC_HDC}
\end{figure}

The Hard Decision is made by verifying if the interval data is entirely less than $ \tau $, comparing the left bound of the information and the right bound of the threshold. This procedure grants that every possible punctual coefficient is less than all possible punctual values belonging to $ \tau $.

% \begin{algorithm}[H]
%     \caption{Hard Decision method.}
%     \label{alg:harddecision}
%     \KwData{$ C[0...n] $ : interval}
%     $ i \leftarrow 0 $ \;
%     \For{$ i < n $}
%     {
%         \If{$ \overline{C[i]} < \underline{\tau} $}
%         {
%             $ C[i] = [0:0] $ \;
%         }
%         $ i = i + 1 $ \;
%     }
% \end{algorithm}

The Soft Decision is made by verifying if most of the interval data is less than the midpoint of $ \tau $, comparing the left bound of the information and the center of the threshold. This procedure grants that most possible punctual coefficients are less than the midpoint of $ \overline{\tau} $ and $ \underline{\tau} $.

% \begin{algorithm}[H]
%     \caption{Soft Decision method.}
%     \label{alg:softdecision}
%     \KwData{$ C[0...n] $ : interval}
%     $ mid\tau =  ( \overline{\tau} + \underline{\tau} ) / 2 $ \;
%     $ i \leftarrow 0 $ \;
%     \For{$ i < n $}
%     {
%     	$ midC =  ( \overline{C[i]} + \underline{C[i]} ) / 2 $ \;
%         \If{$ midC < mid\tau $}
%         {
%             $ C[i] = [0:0] $ \;
%         }
%         $ i = i + 1 $ \;
%     }
% \end{algorithm}

\chapter{Tests and Results}
\label{sec:tests}

The numerical validation of algorithms proposed and described in Section~\ref{sec:optintform} is based on the application of the HWT for image processing. In this manner, the interval parameters for test executions were obtained from punctual values setting degenerated intervals and using them as input to the interval extensions of both decimated and undecimated implementations of the HWT.

The implementation of interval procedures in the Int-HWT library performs the computation of interval error in the process, presenting the widest interval diameter contained in the transformation results. For all tests, the time measurement carried out by executing each function 30 times, mean and standard deviation values were obtained from those times and used to generate the figures presented in the following subsections.

The tests were executed on an Intel~\textsuperscript{\textregistered} Core~\textsuperscript{\texttrademark} i7 950 Processor @ 3.07GHz, 6GB RAM DDR3 @ 1066MHz, Windows 10 OS, compiled using Microsoft Visual C++ Compiler for Visual Studio 2013 on x64 Release compilation target.

\section{Haar Wavelet Transform}

In order to compute tests using the 1D HWT and its interval extension, a vector filled with 1048576 random values is used as input. For tests using 2D HWT algorithms, the input is generated from a 1024x1024 matrix of random values, configuring 1048576 values.

\subsection{One Dimensional}

The results shown in Figure~\ref{fig:Dec1DHWT} describe the performance of both decomposition and composition procedures from Section~\ref{subsec:1DCascadeOpt}, targeting the decimated and undecimated 1D HWT. The data indicate that the decimated algorithms developed in this work are $46\%$ faster than the results found in \cite{stollnitz1995}, when composing the results from decomposing the input. Decomposition, Composition and the combination of both operations, are executed $30\times$, taking $15.46ms$, $16.4317ms$ and $31.1012ms$ with standard deviations of $0.757038ms$, $0.155433ms$ and $0.117835ms$ respectively. The accuracy gain is from $95\%$ up to $99.8\%$, meaning that the developed algorithms generate much more exact results compared to those from the literature. Euclidean Distance (EUC) and Mean Square Error (MSE) are around $99.8\%$ of gain, and Peak-to-Noise Ratio (PSNR) is about $24.4\%$.

The data also indicate that the developed undecimated algorithms are about 30\% slower than the original formulations \cite{IEEE2007} when composing the results from the decomposition step, shown in Figure~\ref{fig:Und1DHWT}. The average times of execution for decomposition, composition and the combination of both are respectively $43.4929ms$, $12.0031ms$ and $55.0647ms$, and   the corresponding  standard deviation are  $0.385967ms$, $0.440633ms$ and $0.371797ms$ . 

The undecimated formulation adds no error to calculations on the composition step, considering that the input does not contain errors, which explains the lack of bars in Figure~\ref{fig:Und1DHWT}. The accuracy gain when performing the combination of both decomposition and composition is around $81\%$. The EUC is about $54.5\%$, and MSE is $79.3\%$, while PSNR is $2.15\%$. These results show that despite the loss of performance the developed algorithms are more accurate than the algorithms from the literature~\cite{IEEE2007}.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=14cm]{Imagens/test1-3.png}
%     \caption{Times of execution and error measurement for the 1D HWT using 1048576 random values.}
%     \label{fig:test1-3}
% \end{figure}
%
\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test1.png}
	\caption{Decimated 1D HWT}
    \label{fig:Dec1DHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test3.png}
	\caption{Undecimated 1D HWT}
	\label{fig:Und1DHWT}
  \end{subfigure}
  \caption{Performance, accuracy and metric gains for the 1D HWT.}
  \label{fig:test1-3}
\end{figure}

\subsection{Two Dimensional}

\subsubsection{Decimated}

Data from Figure~\ref{fig:test2S-NS} show the performance of both standard and non-standard approaches for the decimated 2D HWT. The results presented in \mbox{Figure}~\ref{fig:Dec2DSHWT} show a performance boost of $58,1\%$ during the composition step. The average time of execution of the decomposition, composition and the combination of both are $42.2897ms$, $32.5122ms$ and $74.6253ms$, and their standard deviations are $0.271538ms$, $0.487003ms$ and $0.314095ms$ respectively. The accuracy gain of decomposition, composition and the combination of both are $99.8\%$, $93.5\%$ and $98.3\%$, meaning that the results provided from the developed algorithms are much more exact compared to the literatureliterature~\cite{stollnitz1995}. The EUC, MSE and PSNR metrics are $99.8\%$, $99.9\%$ and $19.2\%$, showing that the developed algorithms are more accurate than the algorithms from the literature~\cite{stollnitz1995}.

Figure~\ref{fig:Dec2DNSHWT} shows the performance, accuracy and metric gains for the decimated 2D non-standard HWT comparing to the original algorithms found in~\cite{stollnitz1995}. The average time of execution for decomposition, composition and both operations are $37.8028ms$, $49.1ms$ and $86.0812ms$, and their standard deviations are respectively $0.985911ms$, $0.362122ms$ and $0.255491ms$. It is important to verify that, as shown in Figure~\ref{fig:2DNormFactors}, the developed non-standard method do not need to perform calculations of $ \sqrt{2} $ due to the implemented algebraic simplifications. Therefore, the corresponding calculations do not increase the error during this process, indicated by accuracy values at $100\%$ for all three methods. Due to the lack of error, the EUC and MSE are also shown at $100\%$. The PSNR metric cannot be calculated due to division by $0$, since it uses MSE as divisor and its value is $0$.
%
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=14cm]{Imagens/test2S-NS.png}
%     \caption{Times of execution and error measurement for the 2D Standard and Non Standard HWT using a 1024x1024 image.}
%     \label{fig:test2S-NS}
% \end{figure}
%
\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test2S.png}
	\caption{Decimated 2D Standard HWT}
    \label{fig:Dec2DSHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test2NS.png}
	\caption{Decimated 2D Non-Standard HWT}
	\label{fig:Dec2DNSHWT}
  \end{subfigure}
  \caption{Times of execution and error measurement for the 2D HWT using 1024x1024 matrix with random values.}
  \label{fig:test2S-NS}
\end{figure}
%
\subsubsection{Undecimated}

Figure~\ref{fig:test4S-NS} shows the performance impact of both standard and non-standard approaches for the undecimated 2D HWT. The performance shown in Figure~\ref{fig:Und2DSHWT} and Figure~\ref{fig:Und2DNSHWT} indicates that the developed algorithms are at least $10\%$ slower than the originals, despite the composition step which has a little advantage of $5\%$ on the standard method.
For the standard approach, the average execution time for decomposition, composition and the combination of both are $195.461ms$, $290.717ms$ and $200.601ms$, and their standard deviations $3.43775ms$, $10.5094ms$, $3.13332ms$. For the non-standard approach execution time averages  are $173.385ms$, $286.755ms$ and $192.677ms$, and their standard deviations are $3.03845ms$, $4.40987ms$ and $2.65613ms$. Both approaches also share the same percentage of accuracy gain, $92.6\%$ and $87.4\%$ respectively for decomposition and the combination of both algorithms performing together. The EUC, MSE and PSNR of both approaches are around $51\%$, $76\%$ and $2\%$ respectively.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=14cm]{Imagens/test4S-NS.png}
%     \caption{Times of execution and error measurement for the 2D Non Standard HWT {\it À-Trous} using a 1024x1024 image.}
%     \label{fig:test4S-NS}
% \end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test4S.png}
	\caption{Undecimated 2D Standard HWT}
    \label{fig:Und2DSHWT}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test4NS.png}
	\caption{Undecimated 2D Non-Standard HWT}
	\label{fig:Und2DNSHWT}
  \end{subfigure}
  \caption{Times of execution and error measurement for the 2D HWT using 1024x1024 matrix with random values on 4 levels of decomposition.}
  \label{fig:test4S-NS}
\end{figure}

\section{Daubechies Wavelet Transform}

\subsection{One Dimensinal}

In order to avaluate the 1D DaWT and its interval extension, 3 tests were are executed using vectors filled with 1048576, 4194304 and 16777216 random values as input respectively. For tests using 2D DaWT algorithms, the inputs were generated from 1024x1024, 2048x2048 and 4096x4096 mtrices of random values.

The results shown in Figure~\ref{fig:test5} describe the performance of both decomposition and composition procedures from Section~\ref{sec:1DDaWT}, targeting the one dimensional DaWT. The data indicate that the developed algorithms are $35\%$, $26\%$ and $26\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing the vectors, and its coefficient of variation (CV) are $13.6\%$, $22.4\%$ and $17.6\%$ respectively. The Composition tests indicates that the developed algorithms are $84\%$, $78\%$ and $68\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $5.7\%$, $5.3\%$ and $9.8\%$. The combination of both Decomposition and Composition operations are $64\%$, $53\%$ and $49\%$ and its CV are $7.5\%$, $9.4\%$ and $9.7\%$.

The interval extension shows that the results are $343$x, $22$x and $203629$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT, its due to the proposed filter presented in Section~\ref{sec:1DDaWT}. The results from EUC are $68.3\%$, $68.3\%$ and $68.3\%$, data from MSE indicate gains of $89.9\%$, $91.4\%$ and $91.9\%$, and PSNR gains are $13.7\%$, $18.1\%$ and $22.4\%$. The results from EUC, MSE and PSNR shows a very good gain over the original formulations of the DaWT.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-1048576.png}
	\caption{Using 1048576 values}
    \label{fig:1DDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-4194304.png}
	\caption{Using 4194304 values}
	\label{fig:1DDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test5-16777216.png}
	\caption{Using 16777216 values}
	\label{fig:1DDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 1D DaWT.}
  \label{fig:test5}
\end{figure}

\subsection{Two Dimensinal}

\subsubsection{Standard}

The results shown in Figure~\ref{fig:test6S} describe the performance of both decomposition and composition procedures from Section~\ref{sec:2DDaWT}, targeting the two dimensional Standard DaWT. The data indicate that the developed algorithms are $25\%$, $23\%$ and $17\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing the vectors, and its coefficient of variation (CV) are $7.5\%$, $10.5\%$ and $2.2\%$ respectively. The Composition tests indicates that the developed algorithms are $63\%$, $56\%$ and $39\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $4\%$, $9.6\%$ and $1.6\%$. The combination of both Decomposition and Composition operations are $44\%$, $40\%$ and $28\%$ and its CV are $3.3\%$, $7.1\%$ and $3.3\%$.

The interval extension shows that the results are $220$x, $18$x and $80236$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT. The results from EUC are $66.4\%$, $70\%$ and $70\%$, data from MSE indicate gains of $88.7\%$, $90.8\%$ and $91.2\%$, and PSNR gains are $3.2\%$, $3.6\%$ and $3.6\%$. The results from EUC, MSE and PSNR shows a very good gain over the original formulations of the DaWT.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-1024.png}
	\caption{Using 1024x1024 matrix}
    \label{fig:2DSDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-2048.png}
	\caption{Using 2048x2048 matrix}
	\label{fig:2DSDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6S-4096.png}
	\caption{Using 4096x4096 matrix}
	\label{fig:2DSDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 2D Standard DaWT.}
  \label{fig:test6S}
\end{figure}

\subsubsection{Non Standard}

The results shown in Figure~\ref{fig:test6NS} describe the performance of both decomposition and composition procedures from Section~\ref{sec:2DDaWT}, targeting the two dimensional Standard DaWT. The data indicate that the developed algorithms are $40\%$, $34\%$ and $21\%$ slower than the results based in~\cite{DAUBTHESIS}, when decomposing the vectors, and its coefficient of variation (CV) are $6.3\%$, $7.1\%$ and $3\%$ respectively. The Composition tests indicates that the developed algorithms are $29\%$, $25\%$ and $15\%$ slower than the algorithms in the literature~\cite{DAUBTHESIS}, its CV are $4.7\%$, $15.6\%$ and $2.3\%$. The combination of both Decomposition and Composition operations are $34\%$, $28\%$ and $18\%$ and its CV are $4\%$, $6.7\%$ and $2.7\%$.

The interval extension shows that the results are $3034$x, $67$x and $5916370$x less accurate, meaning that the developed algorithms generate much larger intervals compared to the original formulation of the DaWT. The results from EUC are $47\%$, $43.3\%$ and $45\%$, data from MSE indicate gains of $71.9\%$, $67.6\%$ and $70\%$, and PSNR gains are $1.8\%$, $1.6\%$ and $1.7\%$. The results from EUC, MSE and PSNR shows a very good gain over the original formulations of the DaWT.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-1024.png}
	\caption{Using 1024x1024 matrix}
    \label{fig:2DNSDaWT-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-2048.png}
	\caption{Using 2048x2048 matrix}
	\label{fig:2DNSDaWT-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\textwidth}
	\includegraphics[width=\textwidth]{Imagens/test6NS-4096.png}
	\caption{Using 4096x4096 matrix}
	\label{fig:2DNSDaWT-3}
  \end{subfigure}
  \caption{Performance and metric results for the 2D Non Standard DaWT.}
  \label{fig:test6NS}
\end{figure}

\chapter{Conclusion}
\label{sec:conclusion}

This work presented an interval implementation of the HWT, for both decimated and undecimated approaches and covering all cases of study, obtaining good results related to error analysis, that is, the optimizations improve accuracy. As consequence of the implemented optimizations our algorithms are faster then the original decimated formulation, but slower than the original undecimated algorithms.

As the wavelet transforms are appropriated to data analysis in contexts that the scales of representation are relevant to the problem, the precision gain obtained with the proposed simplifications in this work represent a significant contribution to this research area.

Further research considers the study of the Daubechies Wavelet Transform together with corresponding parallel and/or distribution Int-DWT library extension, by making use of massive parallel architectural of GPUs and considering CUDA programming language.


\bibliography{bibliografia}
\bibliographystyle{abnt}



\end{document}

\grid
